# Knowledge RAG System - æ€§èƒ½åŸºå‡†æµ‹è¯•
# æµ‹è¯•ç³»ç»Ÿå„ç»„ä»¶çš„æ€§èƒ½åŸºå‡†å’Œä¼˜åŒ–æ•ˆæœ

import asyncio
import json
import statistics
import time
import uuid
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Tuple
from unittest.mock import AsyncMock, Mock, patch

import numpy as np
import pytest
from httpx import AsyncClient


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœæ•°æ®ç±»"""

    operation_name: str
    total_operations: int
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    p50_time: float
    p95_time: float
    p99_time: float
    throughput: float
    success_rate: float
    error_count: int


@pytest.mark.performance
class TestDocumentProcessingBenchmarks:
    """æ–‡æ¡£å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    @pytest.fixture
    def benchmark_client(self, async_client):
        """åŸºå‡†æµ‹è¯•å®¢æˆ·ç«¯"""
        mock_token = "benchmark_token_123"
        async_client.headers.update({"Authorization": f"Bearer {mock_token}"})
        return async_client

    @pytest.mark.asyncio
    async def test_document_upload_benchmark(self, benchmark_client):
        """æ–‡æ¡£ä¸Šä¼ æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        client = benchmark_client

        # æµ‹è¯•ä¸åŒæ–‡ä»¶å¤§å°çš„ä¸Šä¼ æ€§èƒ½
        file_sizes = [
            ("small", 1024),  # 1KB
            ("medium", 102400),  # 100KB
            ("large", 1048576),  # 1MB
            ("xlarge", 5242880),  # 5MB
        ]

        benchmark_results = []

        for size_name, size_bytes in file_sizes:
            print(f"\nğŸ“„ æµ‹è¯• {size_name} æ–‡ä»¶ä¸Šä¼ æ€§èƒ½ ({size_bytes // 1024}KB)")

            # ç”Ÿæˆæµ‹è¯•æ–‡ä»¶å†…å®¹
            content = "æµ‹è¯•å†…å®¹" * (size_bytes // 12)  # å¤§çº¦è¾¾åˆ°ç›®æ ‡å¤§å°
            actual_size = len(content.encode("utf-8"))

            operations = 20  # æ¯ç§å¤§å°æµ‹è¯•20æ¬¡
            response_times = []
            success_count = 0
            error_count = 0

            for i in range(operations):
                try:
                    start_time = time.time()

                    mock_response = {
                        "document_id": f"benchmark_{size_name}_{i}_{uuid.uuid4().hex[:8]}",
                        "upload_status": "success",
                        "file_size": actual_size,
                        "processing_status": "pending",
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 201
                        mock_post.return_value.json.return_value = mock_response

                        files = {
                            "file": (
                                f"benchmark_{size_name}_{i}.txt",
                                content,
                                "text/plain",
                            )
                        }
                        response = await client.post("/documents/upload", files=files)

                    end_time = time.time()
                    response_time = end_time - start_time
                    response_times.append(response_time)

                    if response.status_code == 201:
                        success_count += 1
                    else:
                        error_count += 1

                    # çŸ­æš‚é—´éš”
                    await asyncio.sleep(0.05)

                except Exception as e:
                    error_count += 1
                    print(f"ä¸Šä¼ å‡ºé”™: {e}")

            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            if response_times:
                total_time = sum(response_times)
                avg_time = statistics.mean(response_times)
                min_time = min(response_times)
                max_time = max(response_times)

                # è®¡ç®—ç™¾åˆ†ä½æ•°
                sorted_times = sorted(response_times)
                p50_time = statistics.median(sorted_times)
                p95_time = (
                    sorted_times[int(0.95 * len(sorted_times))]
                    if len(sorted_times) > 1
                    else sorted_times[0]
                )
                p99_time = (
                    sorted_times[int(0.99 * len(sorted_times))]
                    if len(sorted_times) > 1
                    else sorted_times[0]
                )

                throughput = success_count / total_time if total_time > 0 else 0
                success_rate = (success_count / operations) * 100

                result = BenchmarkResult(
                    operation_name=f"upload_{size_name}",
                    total_operations=operations,
                    total_time=total_time,
                    avg_time=avg_time,
                    min_time=min_time,
                    max_time=max_time,
                    p50_time=p50_time,
                    p95_time=p95_time,
                    p99_time=p99_time,
                    throughput=throughput,
                    success_rate=success_rate,
                    error_count=error_count,
                )

                benchmark_results.append(result)

                # è¾“å‡ºç»“æœ
                print(f"   æ–‡ä»¶å¤§å°: {actual_size // 1024}KB")
                print(f"   æ“ä½œæ¬¡æ•°: {operations}")
                print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"   å¹³å‡æ—¶é—´: {avg_time:.3f}ç§’")
                print(f"   P95æ—¶é—´: {p95_time:.3f}ç§’")
                print(f"   ååé‡: {throughput:.2f} æ–‡ä»¶/ç§’")
                print(f"   ä¼ è¾“é€Ÿç‡: {(actual_size / 1024 / avg_time):.1f} KB/ç§’")

        # æ€§èƒ½åŸºå‡†æ–­è¨€
        for result in benchmark_results:
            if "small" in result.operation_name:
                assert (
                    result.avg_time < 0.5
                ), f"å°æ–‡ä»¶ä¸Šä¼ å¹³å‡æ—¶é—´è¿‡é•¿: {result.avg_time:.3f}ç§’"
                assert (
                    result.throughput >= 10.0
                ), f"å°æ–‡ä»¶ä¸Šä¼ ååé‡è¿‡ä½: {result.throughput:.2f}"
            elif "medium" in result.operation_name:
                assert (
                    result.avg_time < 1.0
                ), f"ä¸­ç­‰æ–‡ä»¶ä¸Šä¼ å¹³å‡æ—¶é—´è¿‡é•¿: {result.avg_time:.3f}ç§’"
                assert (
                    result.throughput >= 5.0
                ), f"ä¸­ç­‰æ–‡ä»¶ä¸Šä¼ ååé‡è¿‡ä½: {result.throughput:.2f}"
            elif "large" in result.operation_name:
                assert (
                    result.avg_time < 2.0
                ), f"å¤§æ–‡ä»¶ä¸Šä¼ å¹³å‡æ—¶é—´è¿‡é•¿: {result.avg_time:.3f}ç§’"
                assert (
                    result.throughput >= 2.0
                ), f"å¤§æ–‡ä»¶ä¸Šä¼ ååé‡è¿‡ä½: {result.throughput:.2f}"

            assert (
                result.success_rate >= 95.0
            ), f"{result.operation_name} æˆåŠŸç‡è¿‡ä½: {result.success_rate:.1f}%"

        print(f"\nâœ… æ–‡æ¡£ä¸Šä¼ æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡")

    @pytest.mark.asyncio
    async def test_document_processing_benchmark(self, benchmark_client):
        """æ–‡æ¡£å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        client = benchmark_client

        # æµ‹è¯•ä¸åŒç±»å‹æ–‡æ¡£çš„å¤„ç†æ€§èƒ½
        document_types = [
            ("text", "è¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬æ–‡æ¡£çš„å†…å®¹ã€‚" * 100, "text/plain"),
            (
                "markdown",
                "# æ ‡é¢˜\n\nè¿™æ˜¯**markdown**æ–‡æ¡£å†…å®¹ã€‚\n\n- åˆ—è¡¨é¡¹1\n- åˆ—è¡¨é¡¹2" * 50,
                "text/markdown",
            ),
            (
                "json",
                json.dumps(
                    {
                        "data": [f"item_{i}" for i in range(100)],
                        "metadata": {"type": "test"},
                    }
                ),
                "application/json",
            ),
        ]

        benchmark_results = []

        for doc_type, content, mime_type in document_types:
            print(f"\nğŸ”„ æµ‹è¯• {doc_type} æ–‡æ¡£å¤„ç†æ€§èƒ½")

            operations = 15
            processing_times = []
            success_count = 0
            error_count = 0

            for i in range(operations):
                try:
                    # 1. ä¸Šä¼ æ–‡æ¡£
                    upload_start = time.time()

                    document_id = (
                        f"process_benchmark_{doc_type}_{i}_{uuid.uuid4().hex[:8]}"
                    )

                    mock_upload_response = {
                        "document_id": document_id,
                        "upload_status": "success",
                        "processing_status": "pending",
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 201
                        mock_post.return_value.json.return_value = mock_upload_response

                        files = {
                            "file": (
                                f"benchmark_{doc_type}_{i}.txt",
                                content,
                                mime_type,
                            )
                        }
                        upload_response = await client.post(
                            "/documents/upload", files=files
                        )

                    # 2. è§¦å‘å¤„ç†
                    mock_process_response = {
                        "document_id": document_id,
                        "processing_status": "processing",
                        "estimated_time": 5,
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 200
                        mock_post.return_value.json.return_value = mock_process_response

                        process_data = {
                            "document_id": document_id,
                            "processing_options": {"extract_entities": True},
                        }
                        process_response = await client.post(
                            "/documents/process", json=process_data
                        )

                    # 3. æ¨¡æ‹Ÿç­‰å¾…å¤„ç†å®Œæˆ
                    await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

                    # 4. æ£€æŸ¥å¤„ç†çŠ¶æ€
                    mock_status_response = {
                        "document_id": document_id,
                        "processing_status": "completed",
                        "chunks_count": 10 + (i % 5),
                        "entities_count": 5 + (i % 3),
                        "processing_time": 2.5 + (i % 10) * 0.1,
                    }

                    with patch("httpx.AsyncClient.get") as mock_get:
                        mock_get.return_value.status_code = 200
                        mock_get.return_value.json.return_value = mock_status_response

                        status_response = await client.get(
                            f"/documents/{document_id}/status"
                        )

                    processing_end = time.time()
                    total_processing_time = processing_end - upload_start
                    processing_times.append(total_processing_time)

                    if (
                        upload_response.status_code == 201
                        and process_response.status_code == 200
                        and status_response.status_code == 200
                    ):
                        success_count += 1
                    else:
                        error_count += 1

                    await asyncio.sleep(0.1)

                except Exception as e:
                    error_count += 1
                    print(f"å¤„ç†å‡ºé”™: {e}")

            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            if processing_times:
                total_time = sum(processing_times)
                avg_time = statistics.mean(processing_times)
                min_time = min(processing_times)
                max_time = max(processing_times)

                sorted_times = sorted(processing_times)
                p50_time = statistics.median(sorted_times)
                p95_time = (
                    sorted_times[int(0.95 * len(sorted_times))]
                    if len(sorted_times) > 1
                    else sorted_times[0]
                )
                p99_time = (
                    sorted_times[int(0.99 * len(sorted_times))]
                    if len(sorted_times) > 1
                    else sorted_times[0]
                )

                throughput = success_count / total_time if total_time > 0 else 0
                success_rate = (success_count / operations) * 100

                result = BenchmarkResult(
                    operation_name=f"process_{doc_type}",
                    total_operations=operations,
                    total_time=total_time,
                    avg_time=avg_time,
                    min_time=min_time,
                    max_time=max_time,
                    p50_time=p50_time,
                    p95_time=p95_time,
                    p99_time=p99_time,
                    throughput=throughput,
                    success_rate=success_rate,
                    error_count=error_count,
                )

                benchmark_results.append(result)

                # è¾“å‡ºç»“æœ
                print(f"   æ–‡æ¡£ç±»å‹: {doc_type}")
                print(f"   æ“ä½œæ¬¡æ•°: {operations}")
                print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.3f}ç§’")
                print(f"   P95å¤„ç†æ—¶é—´: {p95_time:.3f}ç§’")
                print(f"   å¤„ç†ååé‡: {throughput:.2f} æ–‡æ¡£/ç§’")

        # æ€§èƒ½åŸºå‡†æ–­è¨€
        for result in benchmark_results:
            assert (
                result.avg_time < 5.0
            ), f"{result.operation_name} å¹³å‡å¤„ç†æ—¶é—´è¿‡é•¿: {result.avg_time:.3f}ç§’"
            assert (
                result.success_rate >= 90.0
            ), f"{result.operation_name} æˆåŠŸç‡è¿‡ä½: {result.success_rate:.1f}%"
            assert (
                result.throughput >= 1.0
            ), f"{result.operation_name} ååé‡è¿‡ä½: {result.throughput:.2f}"

        print(f"\nâœ… æ–‡æ¡£å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡")


@pytest.mark.performance
class TestSearchBenchmarks:
    """æœç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    @pytest.fixture
    def search_client(self, async_client):
        """æœç´¢æµ‹è¯•å®¢æˆ·ç«¯"""
        mock_token = "search_benchmark_token"
        async_client.headers.update({"Authorization": f"Bearer {mock_token}"})
        return async_client

    @pytest.mark.asyncio
    async def test_vector_search_benchmark(self, search_client):
        """å‘é‡æœç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        client = search_client

        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„æœç´¢æŸ¥è¯¢
        search_scenarios = [
            ("simple", "æœºå™¨å­¦ä¹ ", 10),
            ("medium", "æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œç®—æ³•ä¼˜åŒ–", 20),
            (
                "complex",
                "åŸºäºTransformeræ¶æ„çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ä¸ä¼˜åŒ–ç­–ç•¥",
                50,
            ),
        ]

        benchmark_results = []

        for scenario_name, query, result_limit in search_scenarios:
            print(f"\nğŸ” æµ‹è¯• {scenario_name} å‘é‡æœç´¢æ€§èƒ½")

            operations = 25
            search_times = []
            success_count = 0
            error_count = 0

            for i in range(operations):
                try:
                    start_time = time.time()

                    # æ¨¡æ‹Ÿå‘é‡æœç´¢ç»“æœ
                    mock_results = [
                        {
                            "document_id": f"search_doc_{scenario_name}_{i}_{j}",
                            "chunk_id": f"chunk_{j}",
                            "content": f"è¿™æ˜¯ä¸'{query}'ç›¸å…³çš„å†…å®¹ç‰‡æ®µ{j+1}...",
                            "similarity_score": 0.95 - (j * 0.02),
                            "metadata": {
                                "document_title": f"ç›¸å…³æ–‡æ¡£{j+1}",
                                "chunk_index": j,
                                "word_count": 150 + (j * 10),
                            },
                        }
                        for j in range(min(result_limit, 15))
                    ]

                    mock_response = {
                        "query": query,
                        "total_count": result_limit,
                        "results": mock_results,
                        "search_time": 0.05
                        + (len(query) / 1000),  # æ¨¡æ‹Ÿæœç´¢æ—¶é—´ä¸æŸ¥è¯¢å¤æ‚åº¦ç›¸å…³
                        "vector_dimension": 768,
                        "index_size": 10000 + (i * 100),
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 200
                        mock_post.return_value.json.return_value = mock_response

                        search_data = {
                            "query": query,
                            "limit": result_limit,
                            "similarity_threshold": 0.7,
                            "search_type": "vector",
                        }

                        response = await client.post("/vector/search", json=search_data)

                    end_time = time.time()
                    search_time = end_time - start_time
                    search_times.append(search_time)

                    if response.status_code == 200:
                        success_count += 1
                    else:
                        error_count += 1

                    await asyncio.sleep(0.02)

                except Exception as e:
                    error_count += 1
                    print(f"æœç´¢å‡ºé”™: {e}")

            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            if search_times:
                total_time = sum(search_times)
                avg_time = statistics.mean(search_times)
                min_time = min(search_times)
                max_time = max(search_times)

                sorted_times = sorted(search_times)
                p50_time = statistics.median(sorted_times)
                p95_time = (
                    sorted_times[int(0.95 * len(sorted_times))]
                    if len(sorted_times) > 1
                    else sorted_times[0]
                )
                p99_time = (
                    sorted_times[int(0.99 * len(sorted_times))]
                    if len(sorted_times) > 1
                    else sorted_times[0]
                )

                throughput = success_count / total_time if total_time > 0 else 0
                success_rate = (success_count / operations) * 100

                result = BenchmarkResult(
                    operation_name=f"vector_search_{scenario_name}",
                    total_operations=operations,
                    total_time=total_time,
                    avg_time=avg_time,
                    min_time=min_time,
                    max_time=max_time,
                    p50_time=p50_time,
                    p95_time=p95_time,
                    p99_time=p99_time,
                    throughput=throughput,
                    success_rate=success_rate,
                    error_count=error_count,
                )

                benchmark_results.append(result)

                # è¾“å‡ºç»“æœ
                print(f"   æŸ¥è¯¢å¤æ‚åº¦: {scenario_name}")
                print(f"   æŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")
                print(f"   ç»“æœé™åˆ¶: {result_limit}")
                print(f"   æ“ä½œæ¬¡æ•°: {operations}")
                print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"   å¹³å‡æœç´¢æ—¶é—´: {avg_time:.3f}ç§’")
                print(f"   P95æœç´¢æ—¶é—´: {p95_time:.3f}ç§’")
                print(f"   æœç´¢ååé‡: {throughput:.2f} æŸ¥è¯¢/ç§’")

        # æ€§èƒ½åŸºå‡†æ–­è¨€
        for result in benchmark_results:
            if "simple" in result.operation_name:
                assert (
                    result.avg_time < 0.2
                ), f"ç®€å•æœç´¢å¹³å‡æ—¶é—´è¿‡é•¿: {result.avg_time:.3f}ç§’"
                assert (
                    result.throughput >= 20.0
                ), f"ç®€å•æœç´¢ååé‡è¿‡ä½: {result.throughput:.2f}"
            elif "medium" in result.operation_name:
                assert (
                    result.avg_time < 0.5
                ), f"ä¸­ç­‰æœç´¢å¹³å‡æ—¶é—´è¿‡é•¿: {result.avg_time:.3f}ç§’"
                assert (
                    result.throughput >= 10.0
                ), f"ä¸­ç­‰æœç´¢ååé‡è¿‡ä½: {result.throughput:.2f}"
            elif "complex" in result.operation_name:
                assert (
                    result.avg_time < 1.0
                ), f"å¤æ‚æœç´¢å¹³å‡æ—¶é—´è¿‡é•¿: {result.avg_time:.3f}ç§’"
                assert (
                    result.throughput >= 5.0
                ), f"å¤æ‚æœç´¢ååé‡è¿‡ä½: {result.throughput:.2f}"

            assert (
                result.success_rate >= 98.0
            ), f"{result.operation_name} æˆåŠŸç‡è¿‡ä½: {result.success_rate:.1f}%"

        print(f"\nâœ… å‘é‡æœç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡")

    @pytest.mark.asyncio
    async def test_hybrid_search_benchmark(self, search_client):
        """æ··åˆæœç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        client = search_client

        print(f"\nğŸ”€ æµ‹è¯•æ··åˆæœç´¢æ€§èƒ½åŸºå‡†")

        # æ··åˆæœç´¢æŸ¥è¯¢ï¼ˆç»“åˆå…³é”®è¯å’Œå‘é‡æœç´¢ï¼‰
        hybrid_queries = [
            {
                "text_query": "æœºå™¨å­¦ä¹ ç®—æ³•",
                "vector_query": "æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œ",
                "weights": {"text": 0.3, "vector": 0.7},
            },
            {
                "text_query": "è‡ªç„¶è¯­è¨€å¤„ç† NLP",
                "vector_query": "æ–‡æœ¬åˆ†æå’Œç†è§£æŠ€æœ¯",
                "weights": {"text": 0.5, "vector": 0.5},
            },
            {
                "text_query": "è®¡ç®—æœºè§†è§‰ CNN",
                "vector_query": "å›¾åƒè¯†åˆ«å’Œç›®æ ‡æ£€æµ‹",
                "weights": {"text": 0.4, "vector": 0.6},
            },
        ]

        operations = 20
        search_times = []
        success_count = 0
        error_count = 0

        for i in range(operations):
            try:
                start_time = time.time()

                # é€‰æ‹©æŸ¥è¯¢
                query_config = hybrid_queries[i % len(hybrid_queries)]

                # æ¨¡æ‹Ÿæ··åˆæœç´¢ç»“æœ
                mock_text_results = [
                    {
                        "document_id": f"text_result_{i}_{j}",
                        "content": f"æ–‡æœ¬æœç´¢ç»“æœ{j+1}: {query_config['text_query']}",
                        "text_score": 0.9 - (j * 0.05),
                        "match_type": "keyword",
                    }
                    for j in range(8)
                ]

                mock_vector_results = [
                    {
                        "document_id": f"vector_result_{i}_{j}",
                        "content": f"å‘é‡æœç´¢ç»“æœ{j+1}: {query_config['vector_query']}",
                        "vector_score": 0.95 - (j * 0.03),
                        "match_type": "semantic",
                    }
                    for j in range(12)
                ]

                # æ¨¡æ‹Ÿæ··åˆè¯„åˆ†å’Œæ’åº
                combined_results = []
                for j in range(10):
                    text_score = mock_text_results[j % len(mock_text_results)][
                        "text_score"
                    ]
                    vector_score = mock_vector_results[j % len(mock_vector_results)][
                        "vector_score"
                    ]

                    hybrid_score = (
                        text_score * query_config["weights"]["text"]
                        + vector_score * query_config["weights"]["vector"]
                    )

                    combined_results.append(
                        {
                            "document_id": f"hybrid_result_{i}_{j}",
                            "content": f"æ··åˆæœç´¢ç»“æœ{j+1}",
                            "hybrid_score": hybrid_score,
                            "text_score": text_score,
                            "vector_score": vector_score,
                            "match_types": ["keyword", "semantic"],
                        }
                    )

                # æŒ‰æ··åˆè¯„åˆ†æ’åº
                combined_results.sort(key=lambda x: x["hybrid_score"], reverse=True)

                mock_response = {
                    "query": query_config,
                    "total_count": len(combined_results),
                    "results": combined_results,
                    "search_time": 0.12 + (i % 5) * 0.02,
                    "text_search_time": 0.05,
                    "vector_search_time": 0.08,
                    "fusion_time": 0.02,
                }

                with patch("httpx.AsyncClient.post") as mock_post:
                    mock_post.return_value.status_code = 200
                    mock_post.return_value.json.return_value = mock_response

                    search_data = {
                        "text_query": query_config["text_query"],
                        "vector_query": query_config["vector_query"],
                        "weights": query_config["weights"],
                        "limit": 10,
                        "search_type": "hybrid",
                    }

                    response = await client.post("/search/hybrid", json=search_data)

                end_time = time.time()
                search_time = end_time - start_time
                search_times.append(search_time)

                if response.status_code == 200:
                    success_count += 1
                else:
                    error_count += 1

                await asyncio.sleep(0.05)

            except Exception as e:
                error_count += 1
                print(f"æ··åˆæœç´¢å‡ºé”™: {e}")

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        if search_times:
            total_time = sum(search_times)
            avg_time = statistics.mean(search_times)
            min_time = min(search_times)
            max_time = max(search_times)

            sorted_times = sorted(search_times)
            p50_time = statistics.median(sorted_times)
            p95_time = (
                sorted_times[int(0.95 * len(sorted_times))]
                if len(sorted_times) > 1
                else sorted_times[0]
            )
            p99_time = (
                sorted_times[int(0.99 * len(sorted_times))]
                if len(sorted_times) > 1
                else sorted_times[0]
            )

            throughput = success_count / total_time if total_time > 0 else 0
            success_rate = (success_count / operations) * 100

            # è¾“å‡ºç»“æœ
            print(f"   æ“ä½œæ¬¡æ•°: {operations}")
            print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"   å¹³å‡æœç´¢æ—¶é—´: {avg_time:.3f}ç§’")
            print(f"   P95æœç´¢æ—¶é—´: {p95_time:.3f}ç§’")
            print(f"   P99æœç´¢æ—¶é—´: {p99_time:.3f}ç§’")
            print(f"   æœç´¢ååé‡: {throughput:.2f} æŸ¥è¯¢/ç§’")

            # æ€§èƒ½åŸºå‡†æ–­è¨€
            assert success_rate >= 95.0, f"æ··åˆæœç´¢æˆåŠŸç‡è¿‡ä½: {success_rate:.1f}%"
            assert avg_time < 0.8, f"æ··åˆæœç´¢å¹³å‡æ—¶é—´è¿‡é•¿: {avg_time:.3f}ç§’"
            assert p95_time < 1.5, f"æ··åˆæœç´¢P95æ—¶é—´è¿‡é•¿: {p95_time:.3f}ç§’"
            assert throughput >= 8.0, f"æ··åˆæœç´¢ååé‡è¿‡ä½: {throughput:.2f}"

        print(f"\nâœ… æ··åˆæœç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡")


@pytest.mark.performance
class TestQABenchmarks:
    """é—®ç­”æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    @pytest.fixture
    def qa_client(self, async_client):
        """é—®ç­”æµ‹è¯•å®¢æˆ·ç«¯"""
        mock_token = "qa_benchmark_token"
        async_client.headers.update({"Authorization": f"Bearer {mock_token}"})
        return async_client

    @pytest.mark.asyncio
    async def test_qa_response_benchmark(self, qa_client):
        """é—®ç­”å“åº”æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        client = qa_client

        # æµ‹è¯•ä¸åŒç±»å‹å’Œå¤æ‚åº¦çš„é—®é¢˜
        question_categories = [
            {
                "category": "factual",
                "questions": [
                    "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                    "æ·±åº¦å­¦ä¹ çš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "Pythonæ˜¯ä»€ä¹ˆç¼–ç¨‹è¯­è¨€ï¼Ÿ",
                    "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
                    "æ•°æ®ç§‘å­¦åŒ…å«å“ªäº›å†…å®¹ï¼Ÿ",
                ],
                "expected_response_time": 2.0,
            },
            {
                "category": "analytical",
                "questions": [
                    "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                    "ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "å¦‚ä½•é€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ",
                    "æ•°æ®é¢„å¤„ç†åœ¨æœºå™¨å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Ÿ",
                    "è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆå¦‚ä½•è§£å†³ï¼Ÿ",
                ],
                "expected_response_time": 3.5,
            },
            {
                "category": "complex",
                "questions": [
                    "è¯·è¯¦ç»†è§£é‡ŠTransformeræ¶æ„çš„å·¥ä½œåŸç†ï¼Œå¹¶åˆ†æå…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚",
                    "æ¯”è¾ƒä¸åŒçš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•ï¼ˆSGDã€Adamã€RMSpropç­‰ï¼‰ï¼Œåˆ†æå®ƒä»¬çš„é€‚ç”¨åœºæ™¯å’Œæ€§èƒ½ç‰¹ç‚¹ã€‚",
                    "åœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­ï¼Œå¦‚ä½•å¤„ç†æ•°æ®ä¸€è‡´æ€§ã€æ¨¡å‹åŒæ­¥å’Œå®¹é”™ç­‰å…³é”®æŠ€æœ¯æŒ‘æˆ˜ï¼Ÿ",
                    "è§£é‡Šå¼ºåŒ–å­¦ä¹ ä¸­çš„ä»·å€¼å‡½æ•°ã€ç­–ç•¥æ¢¯åº¦å’ŒActor-Criticæ–¹æ³•ï¼Œå¹¶è®¨è®ºå®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„é€‰æ‹©ç­–ç•¥ã€‚",
                ],
                "expected_response_time": 6.0,
            },
        ]

        benchmark_results = []

        for category_info in question_categories:
            category = category_info["category"]
            questions = category_info["questions"]
            expected_time = category_info["expected_response_time"]

            print(f"\nğŸ’¬ æµ‹è¯• {category} é—®ç­”æ€§èƒ½åŸºå‡†")

            response_times = []
            success_count = 0
            error_count = 0
            confidence_scores = []

            for i, question in enumerate(questions):
                # æ¯ä¸ªé—®é¢˜æµ‹è¯•3æ¬¡å–å¹³å‡å€¼
                question_times = []

                for attempt in range(3):
                    try:
                        start_time = time.time()

                        # æ¨¡æ‹Ÿé—®ç­”å“åº”
                        answer_length = len(question) * 2 + (
                            100
                            if category == "factual"
                            else 200 if category == "analytical" else 400
                        )

                        mock_response = {
                            "question": question,
                            "answer": f"è¿™æ˜¯å¯¹'{question[:20]}...'çš„è¯¦ç»†å›ç­”ã€‚"
                            + "è¯¦ç»†å†…å®¹..." * (answer_length // 20),
                            "confidence_score": 0.85 + (i % 10) * 0.01,
                            "sources": [
                                {
                                    "document_id": f"qa_source_{category}_{i}_{j}",
                                    "relevance_score": 0.9 - (j * 0.05),
                                    "chunk_content": f"ç›¸å…³å†…å®¹ç‰‡æ®µ{j+1}...",
                                }
                                for j in range(3 + (i % 3))
                            ],
                            "processing_time": 1.0
                            + (len(question) / 100)
                            + (answer_length / 1000),
                            "tokens_used": len(question) + answer_length,
                            "model_info": {
                                "model_name": "gpt-3.5-turbo",
                                "temperature": 0.7,
                                "max_tokens": 2000,
                            },
                        }

                        with patch("httpx.AsyncClient.post") as mock_post:
                            mock_post.return_value.status_code = 200
                            mock_post.return_value.json.return_value = mock_response

                            qa_data = {
                                "question": question,
                                "max_context_length": 4000,
                                "temperature": 0.7,
                                "include_sources": True,
                            }

                            response = await client.post("/qa/ask", json=qa_data)

                        end_time = time.time()
                        response_time = end_time - start_time
                        question_times.append(response_time)

                        if response.status_code == 200:
                            confidence_scores.append(mock_response["confidence_score"])

                        await asyncio.sleep(0.1)

                    except Exception as e:
                        print(f"é—®ç­”å‡ºé”™: {e}")
                        question_times.append(expected_time * 2)  # è®°å½•è¶…æ—¶

                # è®¡ç®—è¯¥é—®é¢˜çš„å¹³å‡å“åº”æ—¶é—´
                if question_times:
                    avg_question_time = statistics.mean(question_times)
                    response_times.append(avg_question_time)

                    if avg_question_time <= expected_time * 1.5:  # å…è®¸50%çš„æ—¶é—´ç¼“å†²
                        success_count += 1
                    else:
                        error_count += 1

                print(
                    f"   é—®é¢˜ {i+1}: {avg_question_time:.3f}ç§’ ({'âœ“' if avg_question_time <= expected_time else 'âœ—'})"
                )

            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            if response_times:
                total_time = sum(response_times)
                avg_time = statistics.mean(response_times)
                min_time = min(response_times)
                max_time = max(response_times)

                sorted_times = sorted(response_times)
                p50_time = statistics.median(sorted_times)
                p95_time = (
                    sorted_times[int(0.95 * len(sorted_times))]
                    if len(sorted_times) > 1
                    else sorted_times[0]
                )
                p99_time = (
                    sorted_times[int(0.99 * len(sorted_times))]
                    if len(sorted_times) > 1
                    else sorted_times[0]
                )

                throughput = len(questions) / total_time if total_time > 0 else 0
                success_rate = (success_count / len(questions)) * 100
                avg_confidence = (
                    statistics.mean(confidence_scores) if confidence_scores else 0
                )

                result = BenchmarkResult(
                    operation_name=f"qa_{category}",
                    total_operations=len(questions),
                    total_time=total_time,
                    avg_time=avg_time,
                    min_time=min_time,
                    max_time=max_time,
                    p50_time=p50_time,
                    p95_time=p95_time,
                    p99_time=p99_time,
                    throughput=throughput,
                    success_rate=success_rate,
                    error_count=error_count,
                )

                benchmark_results.append(result)

                # è¾“å‡ºç»“æœ
                print(f"\n   ğŸ“Š {category} é—®ç­”æ€§èƒ½ç»Ÿè®¡:")
                print(f"      é—®é¢˜æ•°é‡: {len(questions)}")
                print(f"      æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"      å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}ç§’")
                print(f"      P95å“åº”æ—¶é—´: {p95_time:.3f}ç§’")
                print(f"      å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
                print(f"      ååé‡: {throughput:.2f} é—®é¢˜/ç§’")
                print(f"      æœŸæœ›æ—¶é—´: {expected_time:.1f}ç§’")

        # æ€§èƒ½åŸºå‡†æ–­è¨€
        for result in benchmark_results:
            if "factual" in result.operation_name:
                assert (
                    result.avg_time < 2.5
                ), f"äº‹å®æ€§é—®ç­”å¹³å‡æ—¶é—´è¿‡é•¿: {result.avg_time:.3f}ç§’"
                assert (
                    result.p95_time < 4.0
                ), f"äº‹å®æ€§é—®ç­”P95æ—¶é—´è¿‡é•¿: {result.p95_time:.3f}ç§’"
            elif "analytical" in result.operation_name:
                assert (
                    result.avg_time < 4.0
                ), f"åˆ†ææ€§é—®ç­”å¹³å‡æ—¶é—´è¿‡é•¿: {result.avg_time:.3f}ç§’"
                assert (
                    result.p95_time < 6.0
                ), f"åˆ†ææ€§é—®ç­”P95æ—¶é—´è¿‡é•¿: {result.p95_time:.3f}ç§’"
            elif "complex" in result.operation_name:
                assert (
                    result.avg_time < 7.0
                ), f"å¤æ‚é—®ç­”å¹³å‡æ—¶é—´è¿‡é•¿: {result.avg_time:.3f}ç§’"
                assert (
                    result.p95_time < 10.0
                ), f"å¤æ‚é—®ç­”P95æ—¶é—´è¿‡é•¿: {result.p95_time:.3f}ç§’"

            assert (
                result.success_rate >= 80.0
            ), f"{result.operation_name} æˆåŠŸç‡è¿‡ä½: {result.success_rate:.1f}%"

        print(f"\nâœ… é—®ç­”æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡")


@pytest.mark.performance
class TestSystemBenchmarks:
    """ç³»ç»Ÿæ•´ä½“æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_end_to_end_workflow_benchmark(self, async_client):
        """ç«¯åˆ°ç«¯å·¥ä½œæµæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        client = async_client
        mock_token = "e2e_benchmark_token"
        client.headers.update({"Authorization": f"Bearer {mock_token}"})

        print(f"\nğŸ”„ æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµæ€§èƒ½åŸºå‡†")

        # å®Œæ•´å·¥ä½œæµï¼šä¸Šä¼ æ–‡æ¡£ -> å¤„ç† -> æœç´¢ -> é—®ç­”
        workflows = 10
        workflow_times = []
        success_count = 0
        error_count = 0

        for i in range(workflows):
            try:
                workflow_start = time.time()

                # 1. æ–‡æ¡£ä¸Šä¼ 
                document_content = f"è¿™æ˜¯æµ‹è¯•æ–‡æ¡£{i+1}çš„å†…å®¹ã€‚" * 50
                document_id = f"e2e_benchmark_doc_{i}_{uuid.uuid4().hex[:8]}"

                upload_mock = {
                    "document_id": document_id,
                    "upload_status": "success",
                    "file_size": len(document_content.encode("utf-8")),
                }

                with patch("httpx.AsyncClient.post") as mock_post:
                    mock_post.return_value.status_code = 201
                    mock_post.return_value.json.return_value = upload_mock

                    files = {
                        "file": (f"benchmark_{i}.txt", document_content, "text/plain")
                    }
                    upload_response = await client.post(
                        "/documents/upload", files=files
                    )

                # 2. æ–‡æ¡£å¤„ç†
                process_mock = {
                    "document_id": document_id,
                    "processing_status": "completed",
                    "chunks_count": 5,
                    "processing_time": 2.1,
                }

                with patch("httpx.AsyncClient.post") as mock_post:
                    mock_post.return_value.status_code = 200
                    mock_post.return_value.json.return_value = process_mock

                    process_data = {"document_id": document_id}
                    process_response = await client.post(
                        "/documents/process", json=process_data
                    )

                # 3. æœç´¢æµ‹è¯•
                search_mock = {
                    "query": f"æµ‹è¯•æ–‡æ¡£{i+1}",
                    "results": [
                        {
                            "document_id": document_id,
                            "relevance_score": 0.95,
                            "content": document_content[:200],
                        }
                    ],
                    "total_count": 1,
                }

                with patch("httpx.AsyncClient.post") as mock_post:
                    mock_post.return_value.status_code = 200
                    mock_post.return_value.json.return_value = search_mock

                    search_data = {"query": f"æµ‹è¯•æ–‡æ¡£{i+1}"}
                    search_response = await client.post(
                        "/documents/search", json=search_data
                    )

                # 4. é—®ç­”æµ‹è¯•
                qa_mock = {
                    "question": f"å…³äºæµ‹è¯•æ–‡æ¡£{i+1}çš„é—®é¢˜",
                    "answer": f"è¿™æ˜¯åŸºäºæµ‹è¯•æ–‡æ¡£{i+1}çš„å›ç­”...",
                    "confidence_score": 0.88,
                    "sources": [{"document_id": document_id, "relevance_score": 0.95}],
                }

                with patch("httpx.AsyncClient.post") as mock_post:
                    mock_post.return_value.status_code = 200
                    mock_post.return_value.json.return_value = qa_mock

                    qa_data = {"question": f"å…³äºæµ‹è¯•æ–‡æ¡£{i+1}çš„é—®é¢˜"}
                    qa_response = await client.post("/qa/ask", json=qa_data)

                workflow_end = time.time()
                workflow_time = workflow_end - workflow_start
                workflow_times.append(workflow_time)

                # æ£€æŸ¥æ‰€æœ‰æ­¥éª¤æ˜¯å¦æˆåŠŸ
                if (
                    upload_response.status_code == 201
                    and process_response.status_code == 200
                    and search_response.status_code == 200
                    and qa_response.status_code == 200
                ):
                    success_count += 1
                    print(f"   å·¥ä½œæµ {i+1}: {workflow_time:.3f}ç§’ âœ“")
                else:
                    error_count += 1
                    print(f"   å·¥ä½œæµ {i+1}: {workflow_time:.3f}ç§’ âœ—")

                await asyncio.sleep(0.2)

            except Exception as e:
                error_count += 1
                print(f"å·¥ä½œæµ {i+1} å‡ºé”™: {e}")

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        if workflow_times:
            total_time = sum(workflow_times)
            avg_time = statistics.mean(workflow_times)
            min_time = min(workflow_times)
            max_time = max(workflow_times)

            sorted_times = sorted(workflow_times)
            p50_time = statistics.median(sorted_times)
            p95_time = (
                sorted_times[int(0.95 * len(sorted_times))]
                if len(sorted_times) > 1
                else sorted_times[0]
            )

            throughput = success_count / total_time if total_time > 0 else 0
            success_rate = (success_count / workflows) * 100

            # è¾“å‡ºç»“æœ
            print(f"\n   ğŸ“Š ç«¯åˆ°ç«¯å·¥ä½œæµæ€§èƒ½ç»Ÿè®¡:")
            print(f"      å·¥ä½œæµæ•°é‡: {workflows}")
            print(f"      æˆåŠŸæ•°é‡: {success_count}")
            print(f"      å¤±è´¥æ•°é‡: {error_count}")
            print(f"      æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"      å¹³å‡å®Œæˆæ—¶é—´: {avg_time:.3f}ç§’")
            print(f"      æœ€å¿«å®Œæˆæ—¶é—´: {min_time:.3f}ç§’")
            print(f"      æœ€æ…¢å®Œæˆæ—¶é—´: {max_time:.3f}ç§’")
            print(f"      P50å®Œæˆæ—¶é—´: {p50_time:.3f}ç§’")
            print(f"      P95å®Œæˆæ—¶é—´: {p95_time:.3f}ç§’")
            print(f"      å·¥ä½œæµååé‡: {throughput:.2f} å·¥ä½œæµ/ç§’")

            # æ€§èƒ½åŸºå‡†æ–­è¨€
            assert success_rate >= 90.0, f"ç«¯åˆ°ç«¯å·¥ä½œæµæˆåŠŸç‡è¿‡ä½: {success_rate:.1f}%"
            assert avg_time < 8.0, f"ç«¯åˆ°ç«¯å·¥ä½œæµå¹³å‡æ—¶é—´è¿‡é•¿: {avg_time:.3f}ç§’"
            assert p95_time < 12.0, f"ç«¯åˆ°ç«¯å·¥ä½œæµP95æ—¶é—´è¿‡é•¿: {p95_time:.3f}ç§’"
            assert throughput >= 0.5, f"ç«¯åˆ°ç«¯å·¥ä½œæµååé‡è¿‡ä½: {throughput:.2f}"

        print(f"\nâœ… ç«¯åˆ°ç«¯å·¥ä½œæµæ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡")


def generate_performance_report(benchmark_results: List[BenchmarkResult]) -> str:
    """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    report = "\n" + "=" * 80 + "\n"
    report += "ğŸ“Š Knowledge RAG System - æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n"
    report += "=" * 80 + "\n\n"

    report += f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

    # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
    operation_groups = {}
    for result in benchmark_results:
        operation_type = result.operation_name.split("_")[0]
        if operation_type not in operation_groups:
            operation_groups[operation_type] = []
        operation_groups[operation_type].append(result)

    # ç”Ÿæˆå„ç»„æŠ¥å‘Š
    for group_name, results in operation_groups.items():
        report += f"ğŸ“‹ {group_name.upper()} æ“ä½œæ€§èƒ½:\n"
        report += "-" * 50 + "\n"

        for result in results:
            report += f"  {result.operation_name}:\n"
            report += f"    æ“ä½œæ¬¡æ•°: {result.total_operations}\n"
            report += f"    æˆåŠŸç‡: {result.success_rate:.1f}%\n"
            report += f"    å¹³å‡æ—¶é—´: {result.avg_time:.3f}ç§’\n"
            report += f"    P95æ—¶é—´: {result.p95_time:.3f}ç§’\n"
            report += f"    ååé‡: {result.throughput:.2f} æ“ä½œ/ç§’\n"
            report += "\n"

        report += "\n"

    # æ€»ä½“ç»Ÿè®¡
    total_operations = sum(r.total_operations for r in benchmark_results)
    total_success = sum(
        r.total_operations * r.success_rate / 100 for r in benchmark_results
    )
    overall_success_rate = (
        (total_success / total_operations) * 100 if total_operations > 0 else 0
    )

    report += "ğŸ“ˆ æ€»ä½“æ€§èƒ½ç»Ÿè®¡:\n"
    report += "-" * 30 + "\n"
    report += f"  æ€»æ“ä½œæ•°: {total_operations}\n"
    report += f"  æ€»ä½“æˆåŠŸç‡: {overall_success_rate:.1f}%\n"
    report += f"  æµ‹è¯•åœºæ™¯æ•°: {len(benchmark_results)}\n"

    avg_throughput = statistics.mean([r.throughput for r in benchmark_results])
    report += f"  å¹³å‡ååé‡: {avg_throughput:.2f} æ“ä½œ/ç§’\n"

    report += "\n" + "=" * 80 + "\n"

    return report
