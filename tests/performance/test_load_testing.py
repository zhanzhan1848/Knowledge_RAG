# Knowledge RAG System - è´Ÿè½½å’Œæ€§èƒ½æµ‹è¯•
# æµ‹è¯•ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°

import asyncio
import json
import statistics
import threading
import time
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from typing import Any, Dict, List
from unittest.mock import AsyncMock, Mock, patch

import psutil
import pytest
from httpx import AsyncClient


@pytest.mark.performance
class TestLoadTesting:
    """è´Ÿè½½æµ‹è¯•ç±»"""

    @pytest.fixture
    def performance_metrics(self):
        """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
        return {
            "response_times": [],
            "success_count": 0,
            "error_count": 0,
            "throughput": 0,
            "cpu_usage": [],
            "memory_usage": [],
        }

    @pytest.fixture
    async def load_test_client(self, async_client):
        """è´Ÿè½½æµ‹è¯•å®¢æˆ·ç«¯"""
        # æ¨¡æ‹Ÿè®¤è¯
        mock_token = "load_test_token_123"
        async_client.headers.update({"Authorization": f"Bearer {mock_token}"})
        return async_client

    @pytest.mark.asyncio
    async def test_document_upload_load(self, load_test_client, performance_metrics):
        """æµ‹è¯•æ–‡æ¡£ä¸Šä¼ è´Ÿè½½"""
        client = load_test_client
        concurrent_users = 50
        documents_per_user = 5

        print(
            f"å¼€å§‹è´Ÿè½½æµ‹è¯•: {concurrent_users} å¹¶å‘ç”¨æˆ·ï¼Œæ¯ç”¨æˆ·ä¸Šä¼  {documents_per_user} ä¸ªæ–‡æ¡£"
        )

        async def upload_documents_for_user(user_id: int):
            """å•ä¸ªç”¨æˆ·çš„æ–‡æ¡£ä¸Šä¼ ä»»åŠ¡"""
            user_metrics = {"user_id": user_id, "uploads": [], "errors": []}

            for doc_index in range(documents_per_user):
                try:
                    start_time = time.time()

                    # æ¨¡æ‹Ÿæ–‡æ¡£å†…å®¹
                    document_content = (
                        f"è´Ÿè½½æµ‹è¯•æ–‡æ¡£å†…å®¹ - ç”¨æˆ·{user_id} - æ–‡æ¡£{doc_index}" * 100
                    )

                    mock_response = {
                        "document_id": f"load_doc_{user_id}_{doc_index}_{uuid.uuid4().hex[:8]}",
                        "upload_status": "success",
                        "processing_status": "pending",
                        "file_size": len(document_content.encode("utf-8")),
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 201
                        mock_post.return_value.json.return_value = mock_response

                        files = {
                            "file": (
                                f"load_test_{user_id}_{doc_index}.txt",
                                document_content,
                                "text/plain",
                            )
                        }
                        response = await client.post("/documents/upload", files=files)

                    end_time = time.time()
                    response_time = end_time - start_time

                    if response.status_code == 201:
                        user_metrics["uploads"].append(
                            {
                                "document_id": mock_response["document_id"],
                                "response_time": response_time,
                                "file_size": mock_response["file_size"],
                            }
                        )
                        performance_metrics["success_count"] += 1
                    else:
                        user_metrics["errors"].append(
                            {
                                "status_code": response.status_code,
                                "response_time": response_time,
                            }
                        )
                        performance_metrics["error_count"] += 1

                    performance_metrics["response_times"].append(response_time)

                    # æ¨¡æ‹Ÿç”¨æˆ·é—´éš”
                    await asyncio.sleep(0.1)

                except Exception as e:
                    user_metrics["errors"].append({"error": str(e)})
                    performance_metrics["error_count"] += 1

            return user_metrics

        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        monitoring_task = asyncio.create_task(
            self._monitor_system_resources(performance_metrics, duration=30)
        )

        # æ‰§è¡Œè´Ÿè½½æµ‹è¯•
        start_time = time.time()

        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        user_tasks = [
            upload_documents_for_user(user_id) for user_id in range(concurrent_users)
        ]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        user_results = await asyncio.gather(*user_tasks, return_exceptions=True)

        end_time = time.time()
        total_duration = end_time - start_time

        # åœæ­¢ç›‘æ§
        monitoring_task.cancel()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_requests = concurrent_users * documents_per_user
        success_rate = (performance_metrics["success_count"] / total_requests) * 100
        throughput = performance_metrics["success_count"] / total_duration

        response_times = performance_metrics["response_times"]
        avg_response_time = statistics.mean(response_times) if response_times else 0
        p95_response_time = (
            statistics.quantiles(response_times, n=20)[18]
            if len(response_times) >= 20
            else 0
        )
        p99_response_time = (
            statistics.quantiles(response_times, n=100)[98]
            if len(response_times) >= 100
            else 0
        )

        # æ€§èƒ½æ–­è¨€
        assert success_rate >= 95.0, f"æˆåŠŸç‡è¿‡ä½: {success_rate}%"
        assert avg_response_time < 5.0, f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time}ç§’"
        assert p95_response_time < 10.0, f"P95å“åº”æ—¶é—´è¿‡é•¿: {p95_response_time}ç§’"
        assert throughput >= 10.0, f"ååé‡è¿‡ä½: {throughput} è¯·æ±‚/ç§’"

        # è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
        print(f"\nğŸ“Š æ–‡æ¡£ä¸Šä¼ è´Ÿè½½æµ‹è¯•æŠ¥å‘Š:")
        print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"   æˆåŠŸè¯·æ±‚: {performance_metrics['success_count']}")
        print(f"   å¤±è´¥è¯·æ±‚: {performance_metrics['error_count']}")
        print(f"   æˆåŠŸç‡: {success_rate:.2f}%")
        print(f"   æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        print(f"   ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
        print(f"   P95å“åº”æ—¶é—´: {p95_response_time:.3f}ç§’")
        print(f"   P99å“åº”æ—¶é—´: {p99_response_time:.3f}ç§’")

        if performance_metrics["cpu_usage"]:
            avg_cpu = statistics.mean(performance_metrics["cpu_usage"])
            print(f"   å¹³å‡CPUä½¿ç”¨ç‡: {avg_cpu:.1f}%")

        if performance_metrics["memory_usage"]:
            avg_memory = statistics.mean(performance_metrics["memory_usage"])
            print(f"   å¹³å‡å†…å­˜ä½¿ç”¨ç‡: {avg_memory:.1f}%")

    @pytest.mark.asyncio
    async def test_qa_service_load(self, load_test_client, performance_metrics):
        """æµ‹è¯•é—®ç­”æœåŠ¡è´Ÿè½½"""
        client = load_test_client
        concurrent_users = 30
        questions_per_user = 10

        print(
            f"å¼€å§‹é—®ç­”æœåŠ¡è´Ÿè½½æµ‹è¯•: {concurrent_users} å¹¶å‘ç”¨æˆ·ï¼Œæ¯ç”¨æˆ· {questions_per_user} ä¸ªé—®é¢˜"
        )

        # é¢„å®šä¹‰é—®é¢˜é›†
        question_templates = [
            "ä»€ä¹ˆæ˜¯{}ï¼Ÿ",
            "è¯·è§£é‡Š{}çš„åŸç†",
            "{}æœ‰å“ªäº›åº”ç”¨åœºæ™¯ï¼Ÿ",
            "{}çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•å­¦ä¹ {}ï¼Ÿ",
            "{}çš„å‘å±•å†ç¨‹å¦‚ä½•ï¼Ÿ",
            "{}ä¸å…¶ä»–æŠ€æœ¯çš„åŒºåˆ«ï¼Ÿ",
            "{}çš„æœªæ¥å‘å±•è¶‹åŠ¿ï¼Ÿ",
            "{}åœ¨å®é™…é¡¹ç›®ä¸­å¦‚ä½•åº”ç”¨ï¼Ÿ",
            "{}æœ‰å“ªäº›æœ€ä½³å®è·µï¼Ÿ",
        ]

        topics = ["æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", "è®¡ç®—æœºè§†è§‰", "å¼ºåŒ–å­¦ä¹ "]

        async def ask_questions_for_user(user_id: int):
            """å•ä¸ªç”¨æˆ·çš„é—®ç­”ä»»åŠ¡"""
            user_metrics = {"user_id": user_id, "questions": [], "errors": []}

            for q_index in range(questions_per_user):
                try:
                    start_time = time.time()

                    # ç”Ÿæˆé—®é¢˜
                    template = question_templates[q_index % len(question_templates)]
                    topic = topics[user_id % len(topics)]
                    question = template.format(topic)

                    mock_response = {
                        "question": question,
                        "answer": f"è¿™æ˜¯å¯¹'{question}'çš„è¯¦ç»†å›ç­”ã€‚{topic}æ˜¯ä¸€ä¸ªé‡è¦çš„æŠ€æœ¯é¢†åŸŸ...",
                        "confidence_score": 0.85 + (q_index % 10) * 0.01,
                        "sources": [
                            {
                                "document_id": f"doc_{user_id}_{q_index}",
                                "relevance_score": 0.9,
                            }
                        ],
                        "processing_time": 1.2 + (q_index % 5) * 0.1,
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 200
                        mock_post.return_value.json.return_value = mock_response

                        qa_data = {
                            "question": question,
                            "max_context_length": 4000,
                            "temperature": 0.7,
                        }

                        response = await client.post("/qa/ask", json=qa_data)

                    end_time = time.time()
                    response_time = end_time - start_time

                    if response.status_code == 200:
                        user_metrics["questions"].append(
                            {
                                "question": question,
                                "response_time": response_time,
                                "confidence_score": mock_response["confidence_score"],
                            }
                        )
                        performance_metrics["success_count"] += 1
                    else:
                        user_metrics["errors"].append(
                            {
                                "question": question,
                                "status_code": response.status_code,
                                "response_time": response_time,
                            }
                        )
                        performance_metrics["error_count"] += 1

                    performance_metrics["response_times"].append(response_time)

                    # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
                    await asyncio.sleep(0.2)

                except Exception as e:
                    user_metrics["errors"].append(
                        {"question": question, "error": str(e)}
                    )
                    performance_metrics["error_count"] += 1

            return user_metrics

        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        monitoring_task = asyncio.create_task(
            self._monitor_system_resources(performance_metrics, duration=40)
        )

        # æ‰§è¡Œè´Ÿè½½æµ‹è¯•
        start_time = time.time()

        user_tasks = [
            ask_questions_for_user(user_id) for user_id in range(concurrent_users)
        ]

        user_results = await asyncio.gather(*user_tasks, return_exceptions=True)

        end_time = time.time()
        total_duration = end_time - start_time

        # åœæ­¢ç›‘æ§
        monitoring_task.cancel()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_requests = concurrent_users * questions_per_user
        success_rate = (performance_metrics["success_count"] / total_requests) * 100
        throughput = performance_metrics["success_count"] / total_duration

        response_times = performance_metrics["response_times"]
        avg_response_time = statistics.mean(response_times) if response_times else 0
        p95_response_time = (
            statistics.quantiles(response_times, n=20)[18]
            if len(response_times) >= 20
            else 0
        )

        # æ€§èƒ½æ–­è¨€
        assert success_rate >= 98.0, f"é—®ç­”æœåŠ¡æˆåŠŸç‡è¿‡ä½: {success_rate}%"
        assert avg_response_time < 3.0, f"é—®ç­”å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time}ç§’"
        assert p95_response_time < 6.0, f"é—®ç­”P95å“åº”æ—¶é—´è¿‡é•¿: {p95_response_time}ç§’"
        assert throughput >= 5.0, f"é—®ç­”ååé‡è¿‡ä½: {throughput} è¯·æ±‚/ç§’"

        # è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
        print(f"\nğŸ“Š é—®ç­”æœåŠ¡è´Ÿè½½æµ‹è¯•æŠ¥å‘Š:")
        print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"   æˆåŠŸè¯·æ±‚: {performance_metrics['success_count']}")
        print(f"   å¤±è´¥è¯·æ±‚: {performance_metrics['error_count']}")
        print(f"   æˆåŠŸç‡: {success_rate:.2f}%")
        print(f"   æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        print(f"   ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
        print(f"   P95å“åº”æ—¶é—´: {p95_response_time:.3f}ç§’")

    @pytest.mark.asyncio
    async def test_search_service_load(self, load_test_client, performance_metrics):
        """æµ‹è¯•æœç´¢æœåŠ¡è´Ÿè½½"""
        client = load_test_client
        concurrent_users = 40
        searches_per_user = 8

        print(
            f"å¼€å§‹æœç´¢æœåŠ¡è´Ÿè½½æµ‹è¯•: {concurrent_users} å¹¶å‘ç”¨æˆ·ï¼Œæ¯ç”¨æˆ· {searches_per_user} æ¬¡æœç´¢"
        )

        # é¢„å®šä¹‰æœç´¢å…³é”®è¯
        search_keywords = [
            "æœºå™¨å­¦ä¹ ç®—æ³•",
            "æ·±åº¦ç¥ç»ç½‘ç»œ",
            "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
            "è®¡ç®—æœºè§†è§‰åº”ç”¨",
            "æ•°æ®æŒ–æ˜æ–¹æ³•",
            "äººå·¥æ™ºèƒ½å‘å±•",
            "å¤§æ•°æ®åˆ†æ",
            "äº‘è®¡ç®—æ¶æ„",
            "åŒºå—é“¾æŠ€æœ¯",
            "ç‰©è”ç½‘åº”ç”¨",
            "è¾¹ç¼˜è®¡ç®—",
            "é‡å­è®¡ç®—",
        ]

        async def search_for_user(user_id: int):
            """å•ä¸ªç”¨æˆ·çš„æœç´¢ä»»åŠ¡"""
            user_metrics = {"user_id": user_id, "searches": [], "errors": []}

            for search_index in range(searches_per_user):
                try:
                    start_time = time.time()

                    # é€‰æ‹©æœç´¢å…³é”®è¯
                    keyword = search_keywords[
                        (user_id * searches_per_user + search_index)
                        % len(search_keywords)
                    ]

                    mock_response = {
                        "query": keyword,
                        "total_count": 15 + (search_index % 10),
                        "results": [
                            {
                                "document_id": f"search_doc_{user_id}_{search_index}_{i}",
                                "title": f"ç›¸å…³æ–‡æ¡£{i+1}",
                                "relevance_score": 0.9 - (i * 0.05),
                                "matched_chunks": [
                                    {
                                        "chunk_id": f"chunk_{i}",
                                        "content": f"åŒ…å«'{keyword}'çš„å†…å®¹ç‰‡æ®µ...",
                                        "score": 0.85 - (i * 0.03),
                                    }
                                ],
                            }
                            for i in range(min(5, 15 + (search_index % 10)))
                        ],
                        "search_time": 0.08 + (search_index % 5) * 0.02,
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 200
                        mock_post.return_value.json.return_value = mock_response

                        search_data = {
                            "query": keyword,
                            "limit": 10,
                            "offset": 0,
                            "filters": {},
                        }

                        response = await client.post(
                            "/documents/search", json=search_data
                        )

                    end_time = time.time()
                    response_time = end_time - start_time

                    if response.status_code == 200:
                        user_metrics["searches"].append(
                            {
                                "keyword": keyword,
                                "response_time": response_time,
                                "result_count": mock_response["total_count"],
                            }
                        )
                        performance_metrics["success_count"] += 1
                    else:
                        user_metrics["errors"].append(
                            {
                                "keyword": keyword,
                                "status_code": response.status_code,
                                "response_time": response_time,
                            }
                        )
                        performance_metrics["error_count"] += 1

                    performance_metrics["response_times"].append(response_time)

                    # æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆç»“æœæ—¶é—´
                    await asyncio.sleep(0.15)

                except Exception as e:
                    user_metrics["errors"].append({"keyword": keyword, "error": str(e)})
                    performance_metrics["error_count"] += 1

            return user_metrics

        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        monitoring_task = asyncio.create_task(
            self._monitor_system_resources(performance_metrics, duration=25)
        )

        # æ‰§è¡Œè´Ÿè½½æµ‹è¯•
        start_time = time.time()

        user_tasks = [search_for_user(user_id) for user_id in range(concurrent_users)]

        user_results = await asyncio.gather(*user_tasks, return_exceptions=True)

        end_time = time.time()
        total_duration = end_time - start_time

        # åœæ­¢ç›‘æ§
        monitoring_task.cancel()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_requests = concurrent_users * searches_per_user
        success_rate = (performance_metrics["success_count"] / total_requests) * 100
        throughput = performance_metrics["success_count"] / total_duration

        response_times = performance_metrics["response_times"]
        avg_response_time = statistics.mean(response_times) if response_times else 0
        p95_response_time = (
            statistics.quantiles(response_times, n=20)[18]
            if len(response_times) >= 20
            else 0
        )

        # æ€§èƒ½æ–­è¨€
        assert success_rate >= 99.0, f"æœç´¢æœåŠ¡æˆåŠŸç‡è¿‡ä½: {success_rate}%"
        assert avg_response_time < 1.0, f"æœç´¢å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time}ç§’"
        assert p95_response_time < 2.0, f"æœç´¢P95å“åº”æ—¶é—´è¿‡é•¿: {p95_response_time}ç§’"
        assert throughput >= 15.0, f"æœç´¢ååé‡è¿‡ä½: {throughput} è¯·æ±‚/ç§’"

        # è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
        print(f"\nğŸ“Š æœç´¢æœåŠ¡è´Ÿè½½æµ‹è¯•æŠ¥å‘Š:")
        print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"   æˆåŠŸè¯·æ±‚: {performance_metrics['success_count']}")
        print(f"   å¤±è´¥è¯·æ±‚: {performance_metrics['error_count']}")
        print(f"   æˆåŠŸç‡: {success_rate:.2f}%")
        print(f"   æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        print(f"   ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
        print(f"   P95å“åº”æ—¶é—´: {p95_response_time:.3f}ç§’")

    async def _monitor_system_resources(self, metrics: Dict[str, Any], duration: int):
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        start_time = time.time()

        while time.time() - start_time < duration:
            try:
                # è·å–CPUä½¿ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=1)
                metrics["cpu_usage"].append(cpu_percent)

                # è·å–å†…å­˜ä½¿ç”¨ç‡
                memory = psutil.virtual_memory()
                metrics["memory_usage"].append(memory.percent)

                await asyncio.sleep(1)

            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"ç›‘æ§ç³»ç»Ÿèµ„æºæ—¶å‡ºé”™: {e}")
                break


@pytest.mark.performance
class TestStressTesting:
    """å‹åŠ›æµ‹è¯•ç±»"""

    @pytest.mark.asyncio
    async def test_system_stress_mixed_load(self, load_test_client):
        """æµ‹è¯•æ··åˆè´Ÿè½½ä¸‹çš„ç³»ç»Ÿå‹åŠ›"""
        client = load_test_client

        print("å¼€å§‹ç³»ç»Ÿå‹åŠ›æµ‹è¯•: æ··åˆè´Ÿè½½åœºæ™¯")

        # å®šä¹‰ä¸åŒç±»å‹çš„è´Ÿè½½
        load_scenarios = {
            "document_upload": {"users": 20, "operations": 3},
            "qa_requests": {"users": 30, "operations": 8},
            "search_requests": {"users": 25, "operations": 6},
            "kg_operations": {"users": 10, "operations": 4},
        }

        performance_metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "response_times": [],
            "scenario_metrics": {},
        }

        async def document_upload_scenario(user_id: int, operations: int):
            """æ–‡æ¡£ä¸Šä¼ åœºæ™¯"""
            scenario_metrics = {"success": 0, "errors": 0, "response_times": []}

            for i in range(operations):
                try:
                    start_time = time.time()

                    mock_response = {
                        "document_id": f"stress_doc_{user_id}_{i}",
                        "upload_status": "success",
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 201
                        mock_post.return_value.json.return_value = mock_response

                        files = {
                            "file": (
                                f"stress_test_{user_id}_{i}.txt",
                                "æµ‹è¯•å†…å®¹" * 50,
                                "text/plain",
                            )
                        }
                        response = await client.post("/documents/upload", files=files)

                    response_time = time.time() - start_time
                    scenario_metrics["response_times"].append(response_time)

                    if response.status_code == 201:
                        scenario_metrics["success"] += 1
                    else:
                        scenario_metrics["errors"] += 1

                    await asyncio.sleep(0.1)

                except Exception:
                    scenario_metrics["errors"] += 1

            return scenario_metrics

        async def qa_scenario(user_id: int, operations: int):
            """é—®ç­”åœºæ™¯"""
            scenario_metrics = {"success": 0, "errors": 0, "response_times": []}

            questions = ["ä»€ä¹ˆæ˜¯AIï¼Ÿ", "æœºå™¨å­¦ä¹ åŸç†ï¼Ÿ", "æ·±åº¦å­¦ä¹ åº”ç”¨ï¼Ÿ", "NLPæŠ€æœ¯ï¼Ÿ"]

            for i in range(operations):
                try:
                    start_time = time.time()

                    mock_response = {
                        "answer": "è¿™æ˜¯å‹åŠ›æµ‹è¯•çš„å›ç­”",
                        "confidence_score": 0.9,
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 200
                        mock_post.return_value.json.return_value = mock_response

                        qa_data = {"question": questions[i % len(questions)]}
                        response = await client.post("/qa/ask", json=qa_data)

                    response_time = time.time() - start_time
                    scenario_metrics["response_times"].append(response_time)

                    if response.status_code == 200:
                        scenario_metrics["success"] += 1
                    else:
                        scenario_metrics["errors"] += 1

                    await asyncio.sleep(0.05)

                except Exception:
                    scenario_metrics["errors"] += 1

            return scenario_metrics

        async def search_scenario(user_id: int, operations: int):
            """æœç´¢åœºæ™¯"""
            scenario_metrics = {"success": 0, "errors": 0, "response_times": []}

            keywords = ["æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "AIåº”ç”¨", "æ•°æ®ç§‘å­¦"]

            for i in range(operations):
                try:
                    start_time = time.time()

                    mock_response = {
                        "results": [{"document_id": f"result_{i}", "score": 0.9}],
                        "total_count": 10,
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 200
                        mock_post.return_value.json.return_value = mock_response

                        search_data = {"query": keywords[i % len(keywords)]}
                        response = await client.post(
                            "/documents/search", json=search_data
                        )

                    response_time = time.time() - start_time
                    scenario_metrics["response_times"].append(response_time)

                    if response.status_code == 200:
                        scenario_metrics["success"] += 1
                    else:
                        scenario_metrics["errors"] += 1

                    await asyncio.sleep(0.03)

                except Exception:
                    scenario_metrics["errors"] += 1

            return scenario_metrics

        async def kg_scenario(user_id: int, operations: int):
            """çŸ¥è¯†å›¾è°±åœºæ™¯"""
            scenario_metrics = {"success": 0, "errors": 0, "response_times": []}

            for i in range(operations):
                try:
                    start_time = time.time()

                    mock_response = {
                        "entities": [{"text": "å®ä½“1", "label": "PERSON"}],
                        "relationships": [
                            {"subject": "A", "predicate": "å…³è”", "object": "B"}
                        ],
                    }

                    with patch("httpx.AsyncClient.post") as mock_post:
                        mock_post.return_value.status_code = 200
                        mock_post.return_value.json.return_value = mock_response

                        kg_data = {"document_id": f"stress_doc_{user_id}_{i}"}
                        response = await client.post(
                            "/knowledge-graph/extract-entities", json=kg_data
                        )

                    response_time = time.time() - start_time
                    scenario_metrics["response_times"].append(response_time)

                    if response.status_code == 200:
                        scenario_metrics["success"] += 1
                    else:
                        scenario_metrics["errors"] += 1

                    await asyncio.sleep(0.2)

                except Exception:
                    scenario_metrics["errors"] += 1

            return scenario_metrics

        # åˆ›å»ºæ‰€æœ‰åœºæ™¯çš„ä»»åŠ¡
        all_tasks = []

        # æ–‡æ¡£ä¸Šä¼ ä»»åŠ¡
        for user_id in range(load_scenarios["document_upload"]["users"]):
            task = document_upload_scenario(
                user_id, load_scenarios["document_upload"]["operations"]
            )
            all_tasks.append(("document_upload", task))

        # é—®ç­”ä»»åŠ¡
        for user_id in range(load_scenarios["qa_requests"]["users"]):
            task = qa_scenario(user_id, load_scenarios["qa_requests"]["operations"])
            all_tasks.append(("qa_requests", task))

        # æœç´¢ä»»åŠ¡
        for user_id in range(load_scenarios["search_requests"]["users"]):
            task = search_scenario(
                user_id, load_scenarios["search_requests"]["operations"]
            )
            all_tasks.append(("search_requests", task))

        # çŸ¥è¯†å›¾è°±ä»»åŠ¡
        for user_id in range(load_scenarios["kg_operations"]["users"]):
            task = kg_scenario(user_id, load_scenarios["kg_operations"]["operations"])
            all_tasks.append(("kg_operations", task))

        # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        start_time = time.time()

        # éšæœºæ‰“ä¹±ä»»åŠ¡é¡ºåºä»¥æ¨¡æ‹ŸçœŸå®è´Ÿè½½
        import random

        random.shuffle(all_tasks)

        # æ‰§è¡Œä»»åŠ¡
        tasks_to_run = [task for _, task in all_tasks]
        results = await asyncio.gather(*tasks_to_run, return_exceptions=True)

        end_time = time.time()
        total_duration = end_time - start_time

        # æ±‡æ€»ç»“æœ
        for i, (scenario_name, _) in enumerate(all_tasks):
            if i < len(results) and not isinstance(results[i], Exception):
                result = results[i]

                if scenario_name not in performance_metrics["scenario_metrics"]:
                    performance_metrics["scenario_metrics"][scenario_name] = {
                        "success": 0,
                        "errors": 0,
                        "response_times": [],
                    }

                performance_metrics["scenario_metrics"][scenario_name][
                    "success"
                ] += result["success"]
                performance_metrics["scenario_metrics"][scenario_name][
                    "errors"
                ] += result["errors"]
                performance_metrics["scenario_metrics"][scenario_name][
                    "response_times"
                ].extend(result["response_times"])

                performance_metrics["successful_requests"] += result["success"]
                performance_metrics["failed_requests"] += result["errors"]
                performance_metrics["response_times"].extend(result["response_times"])

        performance_metrics["total_requests"] = (
            performance_metrics["successful_requests"]
            + performance_metrics["failed_requests"]
        )

        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        success_rate = (
            performance_metrics["successful_requests"]
            / performance_metrics["total_requests"]
        ) * 100
        throughput = performance_metrics["successful_requests"] / total_duration

        response_times = performance_metrics["response_times"]
        avg_response_time = statistics.mean(response_times) if response_times else 0

        # å‹åŠ›æµ‹è¯•æ–­è¨€ï¼ˆç›¸å¯¹å®½æ¾çš„æ ‡å‡†ï¼‰
        assert success_rate >= 90.0, f"å‹åŠ›æµ‹è¯•æˆåŠŸç‡è¿‡ä½: {success_rate}%"
        assert (
            avg_response_time < 8.0
        ), f"å‹åŠ›æµ‹è¯•å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time}ç§’"

        # è¾“å‡ºå‹åŠ›æµ‹è¯•æŠ¥å‘Š
        print(f"\nğŸ”¥ ç³»ç»Ÿå‹åŠ›æµ‹è¯•æŠ¥å‘Š:")
        print(f"   æ€»è¯·æ±‚æ•°: {performance_metrics['total_requests']}")
        print(f"   æˆåŠŸè¯·æ±‚: {performance_metrics['successful_requests']}")
        print(f"   å¤±è´¥è¯·æ±‚: {performance_metrics['failed_requests']}")
        print(f"   æˆåŠŸç‡: {success_rate:.2f}%")
        print(f"   æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        print(f"   ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")

        # å„åœºæ™¯è¯¦ç»†æŠ¥å‘Š
        for scenario_name, metrics in performance_metrics["scenario_metrics"].items():
            scenario_success_rate = (
                metrics["success"] / (metrics["success"] + metrics["errors"])
            ) * 100
            scenario_avg_time = (
                statistics.mean(metrics["response_times"])
                if metrics["response_times"]
                else 0
            )

            print(f"\n   ğŸ“‹ {scenario_name} åœºæ™¯:")
            print(f"      æˆåŠŸ: {metrics['success']}, å¤±è´¥: {metrics['errors']}")
            print(f"      æˆåŠŸç‡: {scenario_success_rate:.1f}%")
            print(f"      å¹³å‡å“åº”æ—¶é—´: {scenario_avg_time:.3f}ç§’")


@pytest.mark.performance
class TestMemoryLeakTesting:
    """å†…å­˜æ³„æ¼æµ‹è¯•ç±»"""

    @pytest.mark.asyncio
    async def test_memory_usage_over_time(self, load_test_client):
        """æµ‹è¯•é•¿æ—¶é—´è¿è¡Œçš„å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        client = load_test_client

        print("å¼€å§‹å†…å­˜æ³„æ¼æµ‹è¯•: é•¿æ—¶é—´è¿è¡Œç›‘æ§")

        memory_snapshots = []
        test_duration = 60  # æµ‹è¯•60ç§’

        async def continuous_operations():
            """æŒç»­æ‰§è¡Œæ“ä½œ"""
            operation_count = 0

            while True:
                try:
                    # è½®æ¢ä¸åŒç±»å‹çš„æ“ä½œ
                    if operation_count % 4 == 0:
                        # æ–‡æ¡£ä¸Šä¼ 
                        mock_response = {
                            "document_id": f"mem_test_{operation_count}",
                            "status": "success",
                        }
                        with patch("httpx.AsyncClient.post") as mock_post:
                            mock_post.return_value.status_code = 201
                            mock_post.return_value.json.return_value = mock_response
                            files = {"file": ("test.txt", "å†…å®¹" * 100, "text/plain")}
                            await client.post("/documents/upload", files=files)

                    elif operation_count % 4 == 1:
                        # é—®ç­”è¯·æ±‚
                        mock_response = {"answer": "æµ‹è¯•å›ç­”", "confidence_score": 0.9}
                        with patch("httpx.AsyncClient.post") as mock_post:
                            mock_post.return_value.status_code = 200
                            mock_post.return_value.json.return_value = mock_response
                            await client.post(
                                "/qa/ask",
                                json={"question": f"æµ‹è¯•é—®é¢˜{operation_count}"},
                            )

                    elif operation_count % 4 == 2:
                        # æœç´¢è¯·æ±‚
                        mock_response = {"results": [], "total_count": 0}
                        with patch("httpx.AsyncClient.post") as mock_post:
                            mock_post.return_value.status_code = 200
                            mock_post.return_value.json.return_value = mock_response
                            await client.post(
                                "/documents/search",
                                json={"query": f"æœç´¢{operation_count}"},
                            )

                    else:
                        # çŸ¥è¯†å›¾è°±æ“ä½œ
                        mock_response = {"entities": [], "relationships": []}
                        with patch("httpx.AsyncClient.post") as mock_post:
                            mock_post.return_value.status_code = 200
                            mock_post.return_value.json.return_value = mock_response
                            await client.post(
                                "/knowledge-graph/extract-entities",
                                json={"document_id": f"doc_{operation_count}"},
                            )

                    operation_count += 1
                    await asyncio.sleep(0.1)

                except asyncio.CancelledError:
                    break
                except Exception as e:
                    print(f"æ“ä½œæ‰§è¡Œå‡ºé”™: {e}")
                    await asyncio.sleep(0.1)

        async def memory_monitor():
            """å†…å­˜ç›‘æ§"""
            start_time = time.time()

            while time.time() - start_time < test_duration:
                try:
                    # è·å–å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨æƒ…å†µ
                    process = psutil.Process()
                    memory_info = process.memory_info()

                    snapshot = {
                        "timestamp": time.time() - start_time,
                        "rss": memory_info.rss / 1024 / 1024,  # MB
                        "vms": memory_info.vms / 1024 / 1024,  # MB
                        "percent": process.memory_percent(),
                    }

                    memory_snapshots.append(snapshot)
                    await asyncio.sleep(2)  # æ¯2ç§’é‡‡æ ·ä¸€æ¬¡

                except asyncio.CancelledError:
                    break
                except Exception as e:
                    print(f"å†…å­˜ç›‘æ§å‡ºé”™: {e}")
                    await asyncio.sleep(2)

        # å¯åŠ¨æ“ä½œå’Œç›‘æ§ä»»åŠ¡
        operations_task = asyncio.create_task(continuous_operations())
        monitor_task = asyncio.create_task(memory_monitor())

        # ç­‰å¾…æµ‹è¯•å®Œæˆ
        await asyncio.sleep(test_duration)

        # åœæ­¢ä»»åŠ¡
        operations_task.cancel()
        monitor_task.cancel()

        # åˆ†æå†…å­˜ä½¿ç”¨è¶‹åŠ¿
        if len(memory_snapshots) >= 3:
            initial_memory = statistics.mean([s["rss"] for s in memory_snapshots[:3]])
            final_memory = statistics.mean([s["rss"] for s in memory_snapshots[-3:]])
            memory_growth = final_memory - initial_memory
            growth_rate = memory_growth / test_duration  # MB/ç§’

            max_memory = max(s["rss"] for s in memory_snapshots)
            avg_memory = statistics.mean(s["rss"] for s in memory_snapshots)

            # å†…å­˜æ³„æ¼æ£€æµ‹æ–­è¨€
            assert growth_rate < 1.0, f"å†…å­˜å¢é•¿ç‡è¿‡é«˜: {growth_rate:.3f} MB/ç§’"
            assert memory_growth < 50.0, f"æ€»å†…å­˜å¢é•¿è¿‡å¤§: {memory_growth:.1f} MB"
            assert max_memory < 500.0, f"å³°å€¼å†…å­˜ä½¿ç”¨è¿‡é«˜: {max_memory:.1f} MB"

            # è¾“å‡ºå†…å­˜æµ‹è¯•æŠ¥å‘Š
            print(f"\nğŸ§  å†…å­˜æ³„æ¼æµ‹è¯•æŠ¥å‘Š:")
            print(f"   æµ‹è¯•æ—¶é•¿: {test_duration}ç§’")
            print(f"   åˆå§‹å†…å­˜: {initial_memory:.1f} MB")
            print(f"   æœ€ç»ˆå†…å­˜: {final_memory:.1f} MB")
            print(f"   å†…å­˜å¢é•¿: {memory_growth:.1f} MB")
            print(f"   å¢é•¿ç‡: {growth_rate:.3f} MB/ç§’")
            print(f"   å³°å€¼å†…å­˜: {max_memory:.1f} MB")
            print(f"   å¹³å‡å†…å­˜: {avg_memory:.1f} MB")
            print(f"   é‡‡æ ·æ¬¡æ•°: {len(memory_snapshots)}")

            if growth_rate < 0.1:
                print("   âœ… å†…å­˜ä½¿ç”¨ç¨³å®šï¼Œæ— æ˜æ˜¾æ³„æ¼")
            elif growth_rate < 0.5:
                print("   âš ï¸  å†…å­˜æœ‰è½»å¾®å¢é•¿ï¼Œéœ€è¦å…³æ³¨")
            else:
                print("   âŒ å†…å­˜å¢é•¿è¾ƒå¿«ï¼Œå¯èƒ½å­˜åœ¨æ³„æ¼")

        else:
            print("âš ï¸ å†…å­˜é‡‡æ ·æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæ³„æ¼åˆ†æ")
