#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Knowledge RAG System - æµ‹è¯•æ•°æ®åˆå§‹åŒ–è„šæœ¬

è¯¥è„šæœ¬ç”¨äºåˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒçš„æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
1. æ•°æ®åº“è¿æ¥æµ‹è¯•
2. Redisè¿æ¥æµ‹è¯•
3. Elasticsearchç´¢å¼•åˆ›å»º
4. Neo4jå›¾æ•°æ®åº“åˆå§‹åŒ–
5. æµ‹è¯•æ•°æ®æ’å…¥å’ŒéªŒè¯

ä½œè€…: Knowledge RAG Team
åˆ›å»ºæ—¶é—´: 2024-01-20
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional

import asyncpg
import redis.asyncio as redis
from elasticsearch import AsyncElasticsearch
from neo4j import AsyncGraphDatabase

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/tmp/test_data_init.log')
    ]
)
logger = logging.getLogger(__name__)


class TestDataInitializer:
    """
    æµ‹è¯•æ•°æ®åˆå§‹åŒ–å™¨
    
    è´Ÿè´£åˆå§‹åŒ–æ‰€æœ‰æµ‹è¯•ç¯å¢ƒæ‰€éœ€çš„æ•°æ®å’Œé…ç½®
    """
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        self.database_url = os.getenv('DATABASE_URL', 'postgresql+asyncpg://testuser:testpass123@postgres-test:5432/knowledge_rag_test')
        self.redis_url = os.getenv('REDIS_URL', 'redis://redis-test:6379/0')
        self.elasticsearch_url = os.getenv('ELASTICSEARCH_URL', 'http://elasticsearch-test:9200')
        self.neo4j_uri = os.getenv('NEO4J_URI', 'bolt://neo4j-test:7687')
        self.neo4j_user = os.getenv('NEO4J_USER', 'neo4j')
        self.neo4j_password = os.getenv('NEO4J_PASSWORD', 'testpass123')
        
        # è¿æ¥å¯¹è±¡
        self.pg_pool: Optional[asyncpg.Pool] = None
        self.redis_client: Optional[redis.Redis] = None
        self.es_client: Optional[AsyncElasticsearch] = None
        self.neo4j_driver = None
    
    async def initialize_all(self) -> bool:
        """
        åˆå§‹åŒ–æ‰€æœ‰æµ‹è¯•æ•°æ®
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æµ‹è¯•æ•°æ®...")
            
            # 1. å»ºç«‹æ•°æ®åº“è¿æ¥
            await self._setup_connections()
            
            # 2. éªŒè¯æ•°æ®åº“è¿æ¥
            await self._verify_connections()
            
            # 3. åˆå§‹åŒ–Elasticsearchç´¢å¼•
            await self._setup_elasticsearch_indices()
            
            # 4. åˆå§‹åŒ–Neo4jçº¦æŸå’Œç´¢å¼•
            await self._setup_neo4j_constraints()
            
            # 5. æ’å…¥é¢å¤–çš„æµ‹è¯•æ•°æ®
            await self._insert_additional_test_data()
            
            # 6. éªŒè¯æ•°æ®å®Œæ•´æ€§
            await self._verify_test_data()
            
            logger.info("âœ… æµ‹è¯•æ•°æ®åˆå§‹åŒ–å®Œæˆï¼")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•æ•°æ®åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
        finally:
            await self._cleanup_connections()
    
    async def _setup_connections(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        logger.info("ğŸ“¡ å»ºç«‹æ•°æ®åº“è¿æ¥...")
        
        # PostgreSQLè¿æ¥
        try:
            # ä»URLä¸­æå–è¿æ¥å‚æ•°
            import urllib.parse
            parsed = urllib.parse.urlparse(self.database_url.replace('postgresql+asyncpg://', 'postgresql://'))
            
            self.pg_pool = await asyncpg.create_pool(
                host=parsed.hostname,
                port=parsed.port or 5432,
                user=parsed.username,
                password=parsed.password,
                database=parsed.path.lstrip('/'),
                min_size=1,
                max_size=5
            )
            logger.info("âœ… PostgreSQLè¿æ¥å»ºç«‹æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ PostgreSQLè¿æ¥å¤±è´¥: {e}")
            raise
        
        # Redisè¿æ¥
        try:
            self.redis_client = redis.from_url(self.redis_url)
            logger.info("âœ… Redisè¿æ¥å»ºç«‹æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
            raise
        
        # Elasticsearchè¿æ¥
        try:
            self.es_client = AsyncElasticsearch([self.elasticsearch_url])
            logger.info("âœ… Elasticsearchè¿æ¥å»ºç«‹æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Elasticsearchè¿æ¥å¤±è´¥: {e}")
            raise
        
        # Neo4jè¿æ¥
        try:
            self.neo4j_driver = AsyncGraphDatabase.driver(
                self.neo4j_uri,
                auth=(self.neo4j_user, self.neo4j_password)
            )
            logger.info("âœ… Neo4jè¿æ¥å»ºç«‹æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Neo4jè¿æ¥å¤±è´¥: {e}")
            raise
    
    async def _verify_connections(self):
        """éªŒè¯æ•°æ®åº“è¿æ¥"""
        logger.info("ğŸ” éªŒè¯æ•°æ®åº“è¿æ¥...")
        
        # éªŒè¯PostgreSQL
        async with self.pg_pool.acquire() as conn:
            result = await conn.fetchval('SELECT version()')
            logger.info(f"PostgreSQLç‰ˆæœ¬: {result.split(',')[0]}")
        
        # éªŒè¯Redis
        await self.redis_client.ping()
        info = await self.redis_client.info()
        logger.info(f"Redisç‰ˆæœ¬: {info['redis_version']}")
        
        # éªŒè¯Elasticsearch
        info = await self.es_client.info()
        logger.info(f"Elasticsearchç‰ˆæœ¬: {info['version']['number']}")
        
        # éªŒè¯Neo4j
        async with self.neo4j_driver.session() as session:
            result = await session.run("CALL dbms.components() YIELD name, versions RETURN name, versions[0] as version")
            async for record in result:
                if record['name'] == 'Neo4j Kernel':
                    logger.info(f"Neo4jç‰ˆæœ¬: {record['version']}")
                    break
    
    async def _setup_elasticsearch_indices(self):
        """è®¾ç½®Elasticsearchç´¢å¼•"""
        logger.info("ğŸ“Š è®¾ç½®Elasticsearchç´¢å¼•...")
        
        # æ–‡æ¡£ç´¢å¼•æ˜ å°„
        document_mapping = {
            "mappings": {
                "properties": {
                    "id": {"type": "keyword"},
                    "title": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "content": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "description": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "document_type": {"type": "keyword"},
                    "status": {"type": "keyword"},
                    "user_id": {"type": "keyword"},
                    "tags": {"type": "keyword"},
                    "language": {"type": "keyword"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"},
                    "file_size": {"type": "long"},
                    "word_count": {"type": "integer"},
                    "character_count": {"type": "integer"}
                }
            },
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "ik_max_word": {
                            "type": "standard"
                        },
                        "ik_smart": {
                            "type": "standard"
                        }
                    }
                }
            }
        }
        
        # æ–‡æ¡£å—ç´¢å¼•æ˜ å°„
        chunk_mapping = {
            "mappings": {
                "properties": {
                    "id": {"type": "keyword"},
                    "document_id": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    "content": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "embedding": {
                        "type": "dense_vector",
                        "dims": 1536
                    },
                    "token_count": {"type": "integer"},
                    "character_count": {"type": "integer"},
                    "page_number": {"type": "integer"},
                    "section_title": {"type": "text"},
                    "created_at": {"type": "date"}
                }
            },
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0
            }
        }
        
        # åˆ›å»ºç´¢å¼•
        indices = [
            ("documents", document_mapping),
            ("document_chunks", chunk_mapping)
        ]
        
        for index_name, mapping in indices:
            try:
                # åˆ é™¤å·²å­˜åœ¨çš„ç´¢å¼•
                if await self.es_client.indices.exists(index=index_name):
                    await self.es_client.indices.delete(index=index_name)
                    logger.info(f"åˆ é™¤å·²å­˜åœ¨çš„ç´¢å¼•: {index_name}")
                
                # åˆ›å»ºæ–°ç´¢å¼•
                await self.es_client.indices.create(index=index_name, body=mapping)
                logger.info(f"âœ… åˆ›å»ºç´¢å¼•æˆåŠŸ: {index_name}")
                
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥ {index_name}: {e}")
                raise
    
    async def _setup_neo4j_constraints(self):
        """è®¾ç½®Neo4jçº¦æŸå’Œç´¢å¼•"""
        logger.info("ğŸ”— è®¾ç½®Neo4jçº¦æŸå’Œç´¢å¼•...")
        
        constraints_and_indices = [
            # å®ä½“çº¦æŸ
            "CREATE CONSTRAINT entity_name_type IF NOT EXISTS FOR (e:Entity) REQUIRE (e.name, e.type) IS UNIQUE",
            "CREATE INDEX entity_name_index IF NOT EXISTS FOR (e:Entity) ON (e.name)",
            "CREATE INDEX entity_type_index IF NOT EXISTS FOR (e:Entity) ON (e.type)",
            
            # æ–‡æ¡£çº¦æŸ
            "CREATE CONSTRAINT document_id IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE",
            "CREATE INDEX document_title_index IF NOT EXISTS FOR (d:Document) ON (d.title)",
            "CREATE INDEX document_type_index IF NOT EXISTS FOR (d:Document) ON (d.document_type)",
            
            # ç”¨æˆ·çº¦æŸ
            "CREATE CONSTRAINT user_id IF NOT EXISTS FOR (u:User) REQUIRE u.id IS UNIQUE",
            "CREATE INDEX user_email_index IF NOT EXISTS FOR (u:User) ON (u.email)",
            
            # å…³ç³»ç´¢å¼•
            "CREATE INDEX relationship_type_index IF NOT EXISTS FOR ()-[r:RELATES_TO]-() ON (r.type)",
            "CREATE INDEX relationship_confidence_index IF NOT EXISTS FOR ()-[r:RELATES_TO]-() ON (r.confidence)"
        ]
        
        async with self.neo4j_driver.session() as session:
            for constraint in constraints_and_indices:
                try:
                    await session.run(constraint)
                    logger.info(f"âœ… æ‰§è¡ŒæˆåŠŸ: {constraint[:50]}...")
                except Exception as e:
                    # çº¦æŸå·²å­˜åœ¨çš„é”™è¯¯å¯ä»¥å¿½ç•¥
                    if "already exists" in str(e) or "equivalent" in str(e):
                        logger.info(f"âš ï¸ çº¦æŸå·²å­˜åœ¨: {constraint[:50]}...")
                    else:
                        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {constraint[:50]}... - {e}")
                        raise
    
    async def _insert_additional_test_data(self):
        """æ’å…¥é¢å¤–çš„æµ‹è¯•æ•°æ®"""
        logger.info("ğŸ“ æ’å…¥é¢å¤–çš„æµ‹è¯•æ•°æ®...")
        
        # æ’å…¥Redisæµ‹è¯•æ•°æ®
        await self._insert_redis_test_data()
        
        # æ’å…¥Elasticsearchæµ‹è¯•æ•°æ®
        await self._insert_elasticsearch_test_data()
        
        # æ’å…¥Neo4jæµ‹è¯•æ•°æ®
        await self._insert_neo4j_test_data()
    
    async def _insert_redis_test_data(self):
        """æ’å…¥Redisæµ‹è¯•æ•°æ®"""
        logger.info("ğŸ“¦ æ’å…¥Redisæµ‹è¯•æ•°æ®...")
        
        # ç”¨æˆ·ä¼šè¯æ•°æ®
        session_data = {
            "user_id": "550e8400-e29b-41d4-a716-446655440002",
            "username": "testuser1",
            "role": "user",
            "created_at": datetime.now().isoformat()
        }
        
        await self.redis_client.setex(
            "session:test_session_token_123",
            3600,  # 1å°æ—¶è¿‡æœŸ
            json.dumps(session_data)
        )
        
        # ç¼“å­˜æµ‹è¯•æ•°æ®
        cache_data = {
            "document_id": "660e8400-e29b-41d4-a716-446655440001",
            "title": "æµ‹è¯•æ–‡æ¡£1",
            "content_preview": "è¿™æ˜¯æµ‹è¯•æ–‡æ¡£çš„å†…å®¹é¢„è§ˆ...",
            "cached_at": datetime.now().isoformat()
        }
        
        await self.redis_client.setex(
            "document:660e8400-e29b-41d4-a716-446655440001",
            1800,  # 30åˆ†é’Ÿè¿‡æœŸ
            json.dumps(cache_data)
        )
        
        # é™æµæµ‹è¯•æ•°æ®
        await self.redis_client.setex(
            "rate_limit:user:550e8400-e29b-41d4-a716-446655440002",
            60,  # 1åˆ†é’Ÿè¿‡æœŸ
            "5"  # å·²ä½¿ç”¨5æ¬¡
        )
        
        logger.info("âœ… Redisæµ‹è¯•æ•°æ®æ’å…¥å®Œæˆ")
    
    async def _insert_elasticsearch_test_data(self):
        """æ’å…¥Elasticsearchæµ‹è¯•æ•°æ®"""
        logger.info("ğŸ” æ’å…¥Elasticsearchæµ‹è¯•æ•°æ®...")
        
        # æ–‡æ¡£æ•°æ®
        documents = [
            {
                "id": "660e8400-e29b-41d4-a716-446655440001",
                "title": "æµ‹è¯•æ–‡æ¡£1",
                "content": "è¿™æ˜¯æµ‹è¯•æ–‡æ¡£çš„å†…å®¹ï¼Œç”¨äºéªŒè¯ç³»ç»ŸåŠŸèƒ½ã€‚åŒ…å«äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ç­‰ç›¸å…³æ¦‚å¿µã€‚",
                "description": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£",
                "document_type": "txt",
                "status": "completed",
                "user_id": "550e8400-e29b-41d4-a716-446655440002",
                "tags": ["æµ‹è¯•", "AI", "æœºå™¨å­¦ä¹ "],
                "language": "zh",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "file_size": 1024,
                "word_count": 50,
                "character_count": 150
            },
            {
                "id": "660e8400-e29b-41d4-a716-446655440002",
                "title": "æµ‹è¯•æ–‡æ¡£2",
                "content": "# æµ‹è¯•æ–‡æ¡£\n\nè¿™æ˜¯ä¸€ä¸ªMarkdownæ ¼å¼çš„æµ‹è¯•æ–‡æ¡£ï¼ŒåŒ…å«æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œçš„å†…å®¹ã€‚",
                "description": "è¿™æ˜¯å¦ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£",
                "document_type": "md",
                "status": "completed",
                "user_id": "550e8400-e29b-41d4-a716-446655440002",
                "tags": ["æµ‹è¯•", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ"],
                "language": "zh",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "file_size": 2048,
                "word_count": 80,
                "character_count": 200
            }
        ]
        
        # æ‰¹é‡æ’å…¥æ–‡æ¡£
        for doc in documents:
            await self.es_client.index(
                index="documents",
                id=doc["id"],
                body=doc
            )
        
        # æ–‡æ¡£å—æ•°æ®
        chunks = [
            {
                "id": "chunk_001",
                "document_id": "660e8400-e29b-41d4-a716-446655440001",
                "chunk_index": 0,
                "content": "è¿™æ˜¯æµ‹è¯•æ–‡æ¡£çš„ç¬¬ä¸€ä¸ªå—ï¼ŒåŒ…å«äººå·¥æ™ºèƒ½ç›¸å…³å†…å®¹ã€‚",
                "token_count": 25,
                "character_count": 75,
                "page_number": 1,
                "section_title": "å¼•è¨€",
                "created_at": datetime.now().isoformat()
            },
            {
                "id": "chunk_002",
                "document_id": "660e8400-e29b-41d4-a716-446655440001",
                "chunk_index": 1,
                "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œç”¨äºéªŒè¯ç³»ç»ŸåŠŸèƒ½ã€‚",
                "token_count": 30,
                "character_count": 85,
                "page_number": 1,
                "section_title": "æœºå™¨å­¦ä¹ ",
                "created_at": datetime.now().isoformat()
            }
        ]
        
        # æ‰¹é‡æ’å…¥æ–‡æ¡£å—
        for chunk in chunks:
            await self.es_client.index(
                index="document_chunks",
                id=chunk["id"],
                body=chunk
            )
        
        # åˆ·æ–°ç´¢å¼•
        await self.es_client.indices.refresh(index="documents")
        await self.es_client.indices.refresh(index="document_chunks")
        
        logger.info("âœ… Elasticsearchæµ‹è¯•æ•°æ®æ’å…¥å®Œæˆ")
    
    async def _insert_neo4j_test_data(self):
        """æ’å…¥Neo4jæµ‹è¯•æ•°æ®"""
        logger.info("ğŸ•¸ï¸ æ’å…¥Neo4jæµ‹è¯•æ•°æ®...")
        
        async with self.neo4j_driver.session() as session:
            # æ¸…é™¤å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®
            await session.run("MATCH (n) WHERE n.test_data = true DETACH DELETE n")
            
            # åˆ›å»ºç”¨æˆ·èŠ‚ç‚¹
            await session.run("""
                CREATE (u1:User {
                    id: '550e8400-e29b-41d4-a716-446655440002',
                    username: 'testuser1',
                    email: 'user1@test.com',
                    full_name: 'æµ‹è¯•ç”¨æˆ·1',
                    test_data: true
                })
                CREATE (u2:User {
                    id: '550e8400-e29b-41d4-a716-446655440003',
                    username: 'testuser2',
                    email: 'user2@test.com',
                    full_name: 'æµ‹è¯•ç”¨æˆ·2',
                    test_data: true
                })
            """)
            
            # åˆ›å»ºæ–‡æ¡£èŠ‚ç‚¹
            await session.run("""
                CREATE (d1:Document {
                    id: '660e8400-e29b-41d4-a716-446655440001',
                    title: 'æµ‹è¯•æ–‡æ¡£1',
                    document_type: 'txt',
                    status: 'completed',
                    test_data: true
                })
                CREATE (d2:Document {
                    id: '660e8400-e29b-41d4-a716-446655440002',
                    title: 'æµ‹è¯•æ–‡æ¡£2',
                    document_type: 'md',
                    status: 'completed',
                    test_data: true
                })
            """)
            
            # åˆ›å»ºå®ä½“èŠ‚ç‚¹
            await session.run("""
                CREATE (e1:Entity {
                    id: '990e8400-e29b-41d4-a716-446655440001',
                    name: 'äººå·¥æ™ºèƒ½',
                    type: 'CONCEPT',
                    description: 'äººå·¥æ™ºèƒ½ç›¸å…³æ¦‚å¿µ',
                    confidence: 0.95,
                    test_data: true
                })
                CREATE (e2:Entity {
                    id: '990e8400-e29b-41d4-a716-446655440002',
                    name: 'æœºå™¨å­¦ä¹ ',
                    type: 'CONCEPT',
                    description: 'æœºå™¨å­¦ä¹ æŠ€æœ¯',
                    confidence: 0.90,
                    test_data: true
                })
                CREATE (e3:Entity {
                    id: '990e8400-e29b-41d4-a716-446655440003',
                    name: 'æ·±åº¦å­¦ä¹ ',
                    type: 'CONCEPT',
                    description: 'æ·±åº¦å­¦ä¹ æ–¹æ³•',
                    confidence: 0.88,
                    test_data: true
                })
            """)
            
            # åˆ›å»ºå…³ç³»
            await session.run("""
                MATCH (u1:User {id: '550e8400-e29b-41d4-a716-446655440002'})
                MATCH (d1:Document {id: '660e8400-e29b-41d4-a716-446655440001'})
                MATCH (d2:Document {id: '660e8400-e29b-41d4-a716-446655440002'})
                CREATE (u1)-[:OWNS]->(d1)
                CREATE (u1)-[:OWNS]->(d2)
            """)
            
            await session.run("""
                MATCH (e1:Entity {name: 'äººå·¥æ™ºèƒ½'})
                MATCH (e2:Entity {name: 'æœºå™¨å­¦ä¹ '})
                MATCH (e3:Entity {name: 'æ·±åº¦å­¦ä¹ '})
                CREATE (e1)-[:INCLUDES {confidence: 0.92, test_data: true}]->(e2)
                CREATE (e2)-[:INCLUDES {confidence: 0.89, test_data: true}]->(e3)
            """)
            
            await session.run("""
                MATCH (d1:Document {id: '660e8400-e29b-41d4-a716-446655440001'})
                MATCH (e1:Entity {name: 'äººå·¥æ™ºèƒ½'})
                MATCH (e2:Entity {name: 'æœºå™¨å­¦ä¹ '})
                CREATE (d1)-[:MENTIONS {confidence: 0.85, test_data: true}]->(e1)
                CREATE (d1)-[:MENTIONS {confidence: 0.80, test_data: true}]->(e2)
            """)
        
        logger.info("âœ… Neo4jæµ‹è¯•æ•°æ®æ’å…¥å®Œæˆ")
    
    async def _verify_test_data(self):
        """éªŒè¯æµ‹è¯•æ•°æ®å®Œæ•´æ€§"""
        logger.info("ğŸ” éªŒè¯æµ‹è¯•æ•°æ®å®Œæ•´æ€§...")
        
        # éªŒè¯PostgreSQLæ•°æ®
        async with self.pg_pool.acquire() as conn:
            user_count = await conn.fetchval("SELECT COUNT(*) FROM users")
            doc_count = await conn.fetchval("SELECT COUNT(*) FROM documents")
            msg_count = await conn.fetchval("SELECT COUNT(*) FROM messages")
            
            logger.info(f"PostgreSQLæ•°æ®ç»Ÿè®¡: ç”¨æˆ·={user_count}, æ–‡æ¡£={doc_count}, æ¶ˆæ¯={msg_count}")
            
            if user_count < 4 or doc_count < 3 or msg_count < 3:
                raise ValueError("PostgreSQLæµ‹è¯•æ•°æ®ä¸å®Œæ•´")
        
        # éªŒè¯Redisæ•°æ®
        session_exists = await self.redis_client.exists("session:test_session_token_123")
        cache_exists = await self.redis_client.exists("document:660e8400-e29b-41d4-a716-446655440001")
        
        logger.info(f"Redisæ•°æ®ç»Ÿè®¡: ä¼šè¯å­˜åœ¨={session_exists}, ç¼“å­˜å­˜åœ¨={cache_exists}")
        
        if not session_exists or not cache_exists:
            raise ValueError("Redisæµ‹è¯•æ•°æ®ä¸å®Œæ•´")
        
        # éªŒè¯Elasticsearchæ•°æ®
        doc_count = await self.es_client.count(index="documents")
        chunk_count = await self.es_client.count(index="document_chunks")
        
        logger.info(f"Elasticsearchæ•°æ®ç»Ÿè®¡: æ–‡æ¡£={doc_count['count']}, å—={chunk_count['count']}")
        
        if doc_count['count'] < 2 or chunk_count['count'] < 2:
            raise ValueError("Elasticsearchæµ‹è¯•æ•°æ®ä¸å®Œæ•´")
        
        # éªŒè¯Neo4jæ•°æ®
        async with self.neo4j_driver.session() as session:
            result = await session.run("MATCH (n) WHERE n.test_data = true RETURN labels(n) as labels, count(n) as count")
            neo4j_stats = {}
            async for record in result:
                labels = record['labels']
                count = record['count']
                for label in labels:
                    if label != 'test_data':
                        neo4j_stats[label] = neo4j_stats.get(label, 0) + count
            
            logger.info(f"Neo4jæ•°æ®ç»Ÿè®¡: {neo4j_stats}")
            
            if neo4j_stats.get('User', 0) < 2 or neo4j_stats.get('Entity', 0) < 3:
                raise ValueError("Neo4jæµ‹è¯•æ•°æ®ä¸å®Œæ•´")
        
        logger.info("âœ… æµ‹è¯•æ•°æ®éªŒè¯é€šè¿‡")
    
    async def _cleanup_connections(self):
        """æ¸…ç†æ•°æ®åº“è¿æ¥"""
        logger.info("ğŸ§¹ æ¸…ç†æ•°æ®åº“è¿æ¥...")
        
        try:
            if self.pg_pool:
                await self.pg_pool.close()
            
            if self.redis_client:
                await self.redis_client.close()
            
            if self.es_client:
                await self.es_client.close()
            
            if self.neo4j_driver:
                await self.neo4j_driver.close()
            
            logger.info("âœ… æ•°æ®åº“è¿æ¥æ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†è¿æ¥æ—¶å‡ºé”™: {e}")


async def main():
    """
    ä¸»å‡½æ•°
    """
    initializer = TestDataInitializer()
    
    try:
        success = await initializer.initialize_all()
        if success:
            logger.info("ğŸ‰ æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸï¼")
            sys.exit(0)
        else:
            logger.error("ğŸ’¥ æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥ï¼")
            sys.exit(1)
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­åˆå§‹åŒ–è¿‡ç¨‹")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ğŸ’¥ åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())